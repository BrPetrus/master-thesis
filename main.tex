%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of a document.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/mÂ² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with
               %% `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace
               %% with `colorbold` to use blue for bold text.
  lof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  lot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = mgr,
    department  = Department of Machine Learning and Data Processing,
    author      = Bruno Petrus,
    gender      = m,
    advisor     = {Prof. RNDr. John Smith, CSc.},
    title       = {Segmentation of tunneling nanotubes},
    TeXtitle    = {The Proof of $\mathsf{P}=\mathsf{NP}$},
    keywords    = {keyword1, keyword2, ...},
    TeXkeywords = {keyword1, keyword2, \ldots},
    abstract    = {%
      This is the abstract of my thesis, which can

      span multiple paragraphs.
    },
    thanks      = {%
      These are the acknowledgements for my thesis, which can

      span multiple paragraphs.
    },
    bib         = biblio.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\usepackage{subcaption}
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks
\begin{document}
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. I we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Theses are rumoured to be \enquote{the capstones of education}, so
I decided to write one of my own. If all goes well, I will soon
have a diploma under my belt. Wish me luck!
[TODO: explain semantic segmentation]

\chapter{Data}
The data which we use was taken by our Estonian partners, the data comes from a
sample of wings of the *Drosophila* fly. 

Our work begun when the Estonian partners send us the four different datasets of
the wanted to study phenomena. To study the effects of tunneling nanotubes which
are these thin, long and often time curved structures, they deciced to use wings
of fly larvae. A schematic can be viewed in Figure [TODO]. This is quite
interesting sample as it generaly has, as can be seen in Figure [TODO] and
[TODO] two distinct layers of cells, between which a vertical pillar like
structures are visible. These structures are what we will refer to in this work
as pillars for simplicity. These belong to the [FINISH] part of the wings. How
such data can be acquired and for more information of the process I recommend
looking into the \parencite{Tran2024Programmed} paper.

\section{Acquired Datasets}

The data was captured using fluorescence microscopy. [TODO simple explanation]
and four distinct datasets were acquired. Three of these datasets were captured
with two different channels, larger volume and smaller spatial resolution. All
the datasets are actually part of a time series as it is of quite interest in
looking at the temporal properties of the biological phenomena i.e. do tunnels
stay for multiple steps or are they short-lived? Does their properties change in
time? But that was not the main point of our current research.

The last dataset contains just one channel. The fluorescent dye was selected to
highligh both the pillars and the tunnels as can be seen in Figure [TODO]. As we
can see unlike the previous datasets this was capture at much higher spatial
resolution; therefore, we can see that the dye react mostly in the cell membrane
and they appear hollow. Moreover, the tunnels are much more visible compared to
the previous three datasets, but it only contains a single channel now.

As said before the data was captured by our Estonian partners at [TODO], to
further propel the research they promised us to annotate the three large
datasets, but after several months no labels were given which complicated the
research and work. Hence, we approached our collegues at [TODO what fac.] and
they were kind enough to help us. As we only have labeles to one dataset, in the
rest of this work we will be primarily 

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/t017z0_saturated1percent.png}
    \end{center}
    \caption{First slice of the data at time step 17, after the biologist
    preprocessing pipeline}.
    \label{fig:biologistpreprocessing}
\end{figure}

\section{Labelling and preprocessing}

The biologist created a protocol which they followed by annotating the samples.
They decided to preprocess the data by converting the images from 16bit
grayscale image into 8bit grayscale images, then a 1\%-percentile stretch was
applied to further enhance the images. The result of the preprocessing is
visible in Figure \ref{fig:biologistpreprocessing}. As can be seen on the figure,
this makes the tunnels easier to see at the cost making noise more visible.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/mask000-slice3-saturated.png}
        \caption{}
        \label{fig:gt-slice-3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/mask000-slice3-composite.png}
        \caption{}
        \label{fig:gt-slice-3-composition}
    \end{subfigure}
    \caption{Labeled tunnels by a biologist at 0th time slot on the left. On the
    right the binarized version can be seen overlaid on top of the original
    data.}
    \label{fig:gt-slice-3}
\end{figure}

After the preprocessing is done, the protocol states that picking random time
slots at least 100 instances of tunnels must be annotated, and each frame has to
be fully annotated. According to this protocol the biologists picked two time
steps --- the 0th and 17th step ---  and gave us uniquelly labeled data. The
ground truth created by a biologist for the 0th time slot is visible in the left
part of Figure \ref{fig:gt-slice-3}. In the right part of the mentioned figure
a composition was created with gray data and the binarised ground truth in
magenta colour.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/duplicatedlabel.png}
    \end{center}
    \caption{Maximum projection of the data highlighting problematic tunnel with id 32 located in bottom right quadrant}.
    \label{fig:duplicatedlabel}
\end{figure}

Afterwards the labels were check manually and two cases of label duplicity were
found and fixed, by assigning a unique number to the new connected component.
The flawed labels were 33 located in top right quadrant and label 32 located
bottom right quadrant as can be seen in Figure \ref{fig:duplicatedlabel}.

\chapter{Architectures}
This chapter deals with what neural network architectures were used in
developing the solution.

\section{Preliminary research}
We began by researching similar papers dealing with segmenting nanotubular
structures. Two promising papers we found are ..[TODO]. In

\parencite{Hodneland2006Automated} the researchers are similar to use dealing
with tunnels between cells, but in their case the tunnels form a long straight
lines. In the paper they describe a multistep algorithm which takes employ two
channels, one is for segmenting cell borders and tunnels, while in the other
channel the cells inside are much more visible, hence, this can be used to
better separate cells and tunnel.

As our data has different modality, this approach could not be used. They also
assume that the tunnels are straight which is not true in our case.
The other paper which specificaly mentiones tunneling nanotubes is
\parencite{Ceran2022TNTdetect}. This paper provides consists of three parts.
They describe how to improve the labels and make it easier 

\section{U-Net}
\label{sec:unet}

In 2015 a seminal paper by Rossenberger et al. \parencite{Ronneberger2015} was
published dealing with semantic segmentation of medicine data. The paper
achieved impressive results with very limited dataset. The neural network
architecture is called U-Net and has been behind many models later to come.
[TODO citation]

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/placeholder.png}
    \end{center}
    \caption{A UNet architecure diagram}.
    \label{fig:unetdiagram}
\end{figure}

The key architectural part of the U-Net lays in the introduction of transposed
convolution layers which upsample the result of the encoder. This is placed
after what traditionally like in the neck in VGGNet [TODO citation] was followed
by a classification head now follows the decoder. A diagram can be seen in
Figure \ref{fig:unetdiagram}. As can be seen in the figure, on input it takes an
image and it is followed by a horizontal block formed of two convolutions,
followed by a max pool operation which downsamples the image by a factor of 2.
After this is done numerous times, which depends on the depth of the network set
by the user, additional convolution is applied. The results of the neural
network before the downsample operation is taken is also saved. After the neck
part a series of upsampling operations take place. This is achieved using a
transposed convolution operation and the weights are learned automatically by
the network. Note that other possibilities, like using interpolation in [TODO
citation] was not researched in this work. This upsampling operation doubles the
spatial resolution at the cost of halving the number of channels. Each time such
operation takes place the result of relevant encoder layer is concatonated,
followed by two additional convolutions. Finally we do a final convolution
operation to ouput the right amiunt of classes, which in our case is equal to
two (tunnel or BG/pillar).

The two main reasons for choosing this architecture compared to other was that
the U-Net architecure is well estabilished, widely used, achieves good
performance [TODO citation], but mainly due its nice property that Rossenberger
et al. \parencite{Ronneberger2015} demonstrated that you do not need a huge
dataset to teach the neural network. As our data is quite limited in terms of
size this is a very usefull feature.

The Figure \ref{fig:unetdiagram} is visualising how the UNet architecure can be
used to train a 2D model, but there is not reason why we could not use 3D
versions of convolution, maxpool and transposed upconvolution. 

\subsection{Anisotropic U-Net}

In machine learning it is often tried to increase the model depth to improve the
perfomance of the neural network. This is of course not true universaly as this
can make the network harder to train can cause the network to overtrain [TODO
citation needed]. Since our data is heavily anisotropic we are limited in how
much we can downsample in the z direction, as we have only 7 voxel inside the z
dimension, it is possible to downsample just two times before reaching depth
equal to 2. Moreover, it would probably be reasonable to assert that compressing
all the depth information into a single channel is asking the neural network to
do a lot of work compressing the info.

This problem motivated us to create a new version of the U-Net which uses
anisotropic kernels instead of isotropic kernels for both convolution and the
maxpool operation.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/placeholder.png}
    \end{center}
    \caption{Visualisation of 3D convolution and maxpool operation}.
    \label{fig:visanisooperations}
\end{figure}

As is ilustrated in Figure \ref{fig:visanisooperations} a 3D convolution
uses a 3D kernel to look into a 3D part of the input. By appriopriately padding
the output, we can keep the same dimension in the z axis. Moreover, to prevent
early downsampling in the z dimension we can use anisotropic versions of these
operators. Instead of downsampling by a factor of 2 in all directions, we can
ignore downsampling in the z dimension by setting the kernel size in that
direction equal to 1 [TODO or 0?].

\section{The CSAM module}
\label{sec-csammodule}

This section talks about the channel and spatial attention module (further
refered to as the CSAM) mentioned in the CS-Net paper \parencite{Mou2021}. In
this paper the author tried to improve the performance of UNet architecture by
incorporating attention into the model. This was motivated by using the
architecture for segmenting blood vessels and related biological features and
seeing that one of the problems which was often encoutnered is the fact that
these network had trouble connecting faint blood vessels leading to a
connectivity issues. The attention models helps with connecting them together.

Even though we are not dealing with vessels in our case, the tunnels do visually
resemble some curvilinear structures. We also encountered some of these issues
with connectivity, and as such this might also alleviate the our problems [TODO
reference].

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/placeholder.png}
    \end{center}
    \caption{The CSAM module from Paper [TODO CITE]}.
    \label{fig:csammodule}
\end{figure}

The Figure \ref{fig:csammodule} by \parencite{Mou2021} explain the working
principle behind the CSAM module. It is formed of two paths, the spatial
attention block SB and the channel attention block CAB. One is designed to help
the network find tubular structures inside the spatial direction while the other
is used for helping the network learn inter-channel dependency.

\subsection*{SAB}
The design is motivated by the detection of tubular structures such as blood
vessels in retina images. The detection of such features might require
contextual information rather than purely local information. This lead the
author to use anisotropic kernels 

\subsection*{CAB}
The design ...



\section{Squeeze and excitation block}
\label{sec-seblock}  % consider adding a usenet paragraph
In Paper \parencite{Hu2018} researchers demonstrated how a simple small block
can be inserted inside deep neural networks to improve their performance accross
various datasets and tasks. The researchers postulate that this block helps the
network learn inter channel dependecies [TODO where in paper].

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/t017z0_saturated1percent.png}
    \end{center}
    \caption{Squeeze and exciation block from Paper \parencite{Hu2018}}
    \label{fig:se_diagram}
\end{figure}

The general principle of a squeeze and excitation block is illustrated in Figure
\ref{fig:se_diagram}. The main idea is that we let the network learn
significance of each channel. This is technically done by multiplying the data
$U$ by a number from 0 to 1 in each channel separately.

The squeeze and excitation block consists of four operations: truncate, squeeze,
excitation, and scaling. The truncate operation shown as $F_{TR}$ is a general
convolution operator, which reduces the dimension. This step is not crucial in
the scheme, but it is mainly to illustrate the point, that SE block is usually
built upon some operation. More importantnly, the transdormed data is taken and
run through a squeeze operation, which basically does some global operation in the
whole channel. After that, the excitation operation comes. This operation was
designed to have the following properties: they wanted to create a flexible
operation --- meaning an operation capable of learning non-linear dependecies
--- and they wanted it to be non-mutually exclusive between channels. This is
done by applying two convolutions followed by ReLU and sigmoid respectively.
Note that the first convolution produces output with less than the original
number of channels and the ratio is called the reduction factor. Finally the
last convolution expands the number of channels to the original size. The
comvination of squeeze and excitation allows the network to learn how to weight
each channel (the scale operation mentioned above) by considering multiple
channels.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/placeholder.png}
    \end{center}
    \caption{Placement of SE blocks inside the U-Net by \parencite{Rundo2019}}
    \label{fig:se_unet}
\end{figure}

Another issue is where to put this block, as I already mentioned the truncate
operation, the authors suggest placing it after applying convolution, they also
suggest placement in the Inception and ResNet blocks [TODO citation?]; however,
placement inside a UNet architecture is not mentioned. After further
considerations [TOOD what options] the blocks were placed according to Paper
\parencite{Rundo2019} after every step in the decoder and after each upsampling
step as is shown in Figure \ref{fig:se_unet}.

\chapter{Training}
This chapter explains how the aforementioned neural networks were trained. How
the data was split and handled.

\section{Processing data}
\label{sec:processing_data}
[TODO mention alternatives to supervised learning somewhere]

As mentioned in the previous Chapter [TODO cite], we have 2 time frames of
7x512x512 voxels. As we in some sense only have 2 full images the splitting of
the dataset proved somewhat complicated. We wanted to experiment with using full
3D U-Nets therefore using full sized images would lead to using just one image
for training and one for testing. However, it is no reasoanble to think that
this would be enough to teach the network any reasonable decision. As already
mentioned in Chapter \ref{sec:unet} a nice feature the U-Net architecture is
that it can be learned on much smaller datasets as was demonstrated in the
original U-Net Paper \parencite{Ronneberger2015} [TODO check] and also in the
CS-Net paper \parencite{Mou2021} were few dozens of images were used to train a
neural network to segment medical images. As such a way to extract about a
hundred images was needed.

Splitting the dataset into quadrants would still yield only about 8 7x256x256
images. Importantly, we have about 150 [TODO check] labeled tunnels, which would
suffice the needed number of images. Hence, we decided to cut out the tunnels
and train the network on the these subparts. Note that we still need to run the
network on the whole images, but for training purposes this will suffice.

The splitting logic iterates through every single tunnel and cuts it out. As the
tunnels have different sizes we decided to cut out the bounding boxes of the
tunnels with a set of minimum size. For the minimum size we decided to go for a
7x90x90 shape as that confortably fits majority of tunnels and provides a decent
context around it. [TODO mention expansion after explaining the splits]

\begin{figure}
    % TODO: replace with nicer image.
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/tmp/t017_upperright.png}
    \end{center}
    \caption{}
    \label{fig:upperright}
\end{figure}

It is still important to be somehow able to split the dataset into the train,
test and evalution subset. As the dataset is quite limited choosing a random
sybpart to evaluate the perfomance is not ideal. The various parts of the
dataset do not have the same number of tunnels, tunnels of the same morphology
and the same amount of noise and irrelevant structures. For example in the upper
right corner in Figure \ref{fig:upperright} we can find some irrelevant
structures which are not present in the rest of the dataset.

%As somehowe quantifying the ideal split would be a very demanding task we 
Somehow quantifying the ideal split would be a very challenging and unclear task
so we are always doing something similar to a leave one out [TODO cite] cross
validation. Each time four different models are trained and in each run a
different quadrant is considered to be the testing subset and the remaining
three quandarnt are used for training the model.

This is still quite challenging to do properly as not every tunnel lies neatly
inside one of the quadrants. A split that is used in training is visualised in
Figure [TODO] where a possible split is shown in two colours.

The decision if a particular tunnel belongs in test or train subset is made by
the following decision. [TODO check the numbers and finalise this section.]

\section{Training and Running}


\subsection{Loss funtions}
Since we can look at finding tunneling nanotubes as a binary classification
problem applied at every pixel, it naturally comes to use a binary cross-entropy
loss to teach the network to learn to segment the images. However, our dataset
is very heavily imbalanced as the data contains a lot more negative examples. To
somewhat help the training to pay more attention to the tunnels, the
cross-entropy was weighted by a factor or 50, which was estimated by taking the
calculating the ration of pixels classified as background and pixels assigned to
some tunnel by the biologist.

To further improve the performance of the training, we got inspired CS-Net paper
which uses Dice loss, and some papers claim that the combination of Dice loss
and cross-entropy loss lead to better performance in many segmentation tasks
\parencite{ma2020segmentationlossodyssey}. By using Dice loss it is meant to
optimize the value of the Dice coefficient. This is defined as taking
$L_{Dice} = 1 - \text{Dice}$, meaning that as we get segmentation results closer
to the ideal mask, we get closer to 0.

% TODO: full loses


For the loss function we were inspired by [TODO cite
the loss paper] and by CS-Net \parencite{Mou2021} to try a mix of Dice loss anda
binary cross-entropy loss. Since we were not sure about how to weight these
losses we went by setting their weights to be equal. 

To somewhat alleviate the
issue with the uneven positive and negative labels in the labels, we counted how
many positive and how negative pixels are in the whole dataset, and arrived at
a 50 [TODO check] ratio of these two classes.

[TODO defi. of CE]
[TODO defi. of Dice]
[TODO defi. of Jaccard]

\subsection*{Optimizers, learning rates, regularization}
The neural network was trained using Adam optimzizer, but soon switched as we
are also using weight decay, equal to 1e-4, it is commonly advised to use the
AdamW variant as it correctly decouples the weight decay from the gradient
updates \parencite{loshchilov2019}. However, in practice no vast effect was
observed during out testing.

During the development of the training loop we observed that setting the
learning rate to 0.001 [TODO check] and weight decay to [TODO check] achieved a
very stable learning. This was when using smaller windows size, and the overall
loop was not quite as robust; however, as the development progress the learning
stayed well behaved and no experimentation on the values was done. Since the
Adam family of optimizers change the learning rate adaptively per neuron we did
not see the benefit of rerunning all evaluations also for different learning
strategies considering the time requirements to perform such a study.

\subsection*{Training the network}

As was mentioned in Section \ref{sec:processing_data} the overall size of the
dataset was rather tiny compromising of just a few images. To alleviate this we
instead started focusing on training a network on smaller patches of 7x64x64
size.

A common problem with training neural networks is their capacity to overtrain
on the training part of the dataset. This often times leads to poor
generalisation performance. This is especiialy true fro really small datasets
such as ours. One of the common ways to deal with this problem, as is mentioned
in the original U-Net paper is the use of augmentations
\parencite{Ronneberger2015} to artificially enlarge the training set. In our
case we decided that we will use only augmentations which do not require
interpolation. The reason is that unlike many other tasks, we are potentially
dealing with quite thin structres, which could easily be significantly supressed
or blured and no longer be seen.

In each batch, a different set of transforms are used. This works by setting a
random probability of this particular transform to be applyied. For the training
we are:
\begin{itemize}
    \item{Adding random noise with a Gaussian distribution of mean 0 and 0.01
        standard deviation with probability 0.5, which adds very faint noise.
        This is a relatively low amount of noise and was chosen during the
        development of the network as we did not want to change the images to
        look completely different to the orignal ones.}
    \item{Random flipping in every axes.}
    \item{Random spatial crop which always crops to the preset size 7x64x64 from
        any other size.}
\end{itemize}
A possible problem of using small tiles centered at tunnels is that we might
introduce unwanted bias into the network, as it might learn to always try to
find the tunnel in the middle. The design designation behind using random
spatial crop was to try to suppress this, as the program now randomly finds a
crop of the wanted size, which reduces this problem.

Given all the mentioned settings the 


% [TODO mention inside the intro chapter or architecture]
% When training
%networks using supervised learning approach we usualy have hundreds or more
%images [TODO CITE SOME classification papers], which we unfortunately do not
%have. But, one of the nice 

%it is not possible to simply use one timeframe for training
%and one for testing 

%splitting and stuff
how the neural networks were trained.


\section{Running the network}
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/tile0px.png}
        \caption{0px overlap}
        \label{fig:tile0px}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/tile10px.png}
        \caption{10px overlap}
        \label{fig:tile10px}
    \end{subfigure}
    \caption{Visualisation of overlaps for 0 and 10px overlaps.}
    \label{fig:tiling}
\end{figure}

Since our neural network works on square cutouts of 7x64x64 size, we decided to
solve this problem by tiling the original images and then stitching them
together. This overlap-tile strategy is used in several papers, such as
\parencite{Ronneberger2015} [TODO]. There are roughly two strategies for
handling the edges. In works such as \parencite{Ronneberger2015}, the neural
network's prediction is smaller than the original size since they do not pad the
intermediate results. Then the sampling of images is done in such a way that
every output pixel has a prediction.

Since our crop size is rather small, given the problems with dataset size
described in [TODO], we decided to use padding and always output the same size,
and to have a configurable overlap size between the neighbouring tiles. The idea
is better visualised in Figure \ref{fig:tiling}, where the various overlaps are
visualised.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/t000-3-0verlap.png}
        \caption{0px overlap}
        \label{fig:stitchingoverlap0}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/t000-3-30overlap.png}
        \caption{30px overlap}
        \label{fig:stitchingoverlap30}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/t000-3-orig-stretched.png}
        \caption{Original image (stretched).}  % todo: maybe mention the quad
    \end{subfigure}
    \caption{This figure shows how choosing different amount of overlap in
    pixels has an effect on suppresing some of the artifacts at the edges of
    stitched tiles.}
    \label{fig:stitchingoverlap}
\end{figure}

%todo how to cite subfigure?

The idea to use a configurable overlap size came from the following observation.
In Figure \ref{fig:stitchingoverlap0}, several sharp artefacts can be seen at
places where the tiles were touching, as there was some leftover information
from neighbouring tiles, but not a good view of the whole object. The network
probably got confused and mistook the feature for a very short tunnel. This led
us to the idea that if we use larger overlaps and aggregate them, these effects
on boundaries might get fixed. As can be seen in the figure, using a larger
overlap and averaging the predictions suppresses a lot of these artefacts.
% todo in discussion mention that maybe this was not the best idea, maybe we
% should have chosen larger context. The context x precision tradeoff.

For the purpose of aggregating the overlaps, simple averaging was chosen due to
its simplicity. The main idea was, that if the network predicts a false tunnel
on the edge, due to the limited context, it might assign a high probability
there, but as the other tile has more context, it will say that the tunnel is
there with smaller probability; hence, by averagint them we will get hopefully
small enough probability that the artifact is removed.

Interestingly, some new artifacts appear, which at first glance might seem
strange, but if we compare the locations of some of these artifacts it becomes
clear they correspond to various noise inside the original input.
% todo: add arrow or something

% TODO: consider drawing there
Even after this step, there still might be some leftover artefacts, as can be
seen in Figure \ref{fig:stitchingoverlap}, which is not ideal, but these
artefacts are generally small from the way they arrived. This leads us to how
the predictions are processed.

\section{Postprocessing}
\label{sec:postprocessing}

As mentioned in the previous section, even after applying the overlap strategy,
some artefacts remain. Moreover, some artefacts actually closely resemble thin
structures but are not true tunnels. Many of these false tunnels seem to be
small and not connected to any cell, nor really have a 3D structure. Therefore,
we decided that an easy filtering would be using their volume.

We can think of the tunnels as individual connected components. In general, only
if at some z height the tunnel is not from a single connected component we are
assuming that this will hold if we are looking at the tunnels in 3D instead.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/histogramofsizes.png}
    \end{center}
    \caption{Histogram of the volume (in pixels) of all labeled tunneling
    nanotubes with median and mean highlighted.}.
    \label{fig:histogramoflabelsizes}
\end{figure}

Given this assumption, we can filter the tunnels also based on their sizes. I
chose the value of 100 pixels, realising that some of the tiny tunnels might get
removed, but as will later be shown, the network tends to find more tunnels than
the biologist. Hence, I believe this to be a reasonable assumption to make.
Finally, the predictions are thresholded by the value of 0.5. This was chosen as
a reasonable threshold.
% todo: mention in discussion that this prob. could have been more studied

\chapter{Implementation}

\section{Used technologies}
Python, mlflow, Bash, uv, the shuffling, setting explicit seeds, issues with
batch vs loss, implementing the loss functions in pytorch
implementing the squeeze and excitation.

\section{Environment}

\subsection{Justification for the used technologies}
The previously described training and preprocessing in previous chapters was
implemented as collection of Bash scripts, python scripts and a python module.
All of the code is available as part of the master thesis alongside several of
the trained networks for the user.

The Python environment is packaged and its dependecies are resolved by a tool
cold uv \parencite{uv} developed by a company called Astral, who specialises on
creating modern automatic tooling for Python. The environment is described using
a TOML file available and list a number of dependencies. Altough the project is
quite recent compared to others like Poetry, uv is significanlty faster.

For convenience I am also including a \texttt{requirements.txt} which was automatically generated
from a virtual Python environemnt, but the officially supported version is using
the uv package manager. The environment can then be created in two ways: if the
user runs a computer with a GPU, the working environmnet is creating using the
command \texttt{uv sync --extra torch-gpu} which assumes that CUDA version 12.4
is installed.

I decided to use Python to develop the training and evaluation scripts, while
handling the pipelines around it with Bash. Altough Python is an interpreted and
dynamically typed language and therefore is comparably slower than other lower
level languages traditionally used in image processing like C++, its nature
allows it to rapidly prototype new application. Moreover, many packages call
code written in other languages such as C++, Fortran or C, to somewhat alleviate
the performance problem. Lastly, Python has become a widely adopted language in
the area of neural networks, and many researchers publish their code in Python.

As was already mentioned even though the majority of code is written in Python
it is quite cumbersome to create a single Python script which handles both
training and evaluation, especially if something changes in one of them. This
fundamentally breaks the famous principle in software architecture called the
separation of concers. As such a bash script is provided in the code archive
under \texttt{scripts} called \texttt{run.sh} which is the main entrypoint for
running the code.

As a lot of the evaluation is concerned with doing proper cross-validation and
then averaging the results it was easier to create a script which is easily
configured using a text editor or by providing and parsing CLI arguments and the
averaging and choosing different datasets should not be a responsibility of the
Python script. As such, creating directories, running code, aggregating results
and parsing arguments is a natural job for a scripting language such as Bash.
The obvious disadvantage is that it is limited to the Unix and Unix like
operating systems.

As for the other tooling, here I list the most important tools:
\begin{itemize}
    \item{MLflow is tool for handling machine learning pipelines, it allows to
        track experiments, compare results, save and load models, deploy models
        etc. In this work it was used to create a nice interface to keep track
        of various experiments.}  % cite https://mlflow.org/docs/latest/ml/
    \item{ruff linter TODO cite is an extremely fast Python linter which was
        developed by the company Astra. It automatically formats code and can
        even fix some of the basic lint errors.}
    \item{numpy TODO CITE has become one of the fundamental packages in
        scientific computing in Python ecosystem. It provides a very efficient
        implementation of} 
    \item{scikit-learn provides a great framework for building machine learning
        models not just deep learning models. This was used mainly for splitting
        datasets and using their framework for handling batching.}  % TODO: https://scikit-learn.org/stable/index.html
    \item{scikit-image is a package providing various algorithms for processing
        images e.g. segmentation algorithms, point transform, mathematical
        mopohlogy. I used it to handle the post-processing of masks and to find
        connected components.} % TODO: https://scikit-image.org/
    \item{Pytorch is a popular deep learning framework with focus on flexibility
        and speed, this allows the user to create a well performing neural
        network very fast. It automatically creates computational graphs
        allowing it to compute gradient upgrades efficiently. It handles
        connecting to various hardware like CPU and GPUs automatically. This has
        allowed PyTorch to become one of the most used packages in the Python
        ecostsem and is why I chose it to develop the program. TODO CITE} % https://pytorch.org/projects/pytorch/ 
    \item{MONAI is another opensource framework for building neural networks on
        top of PyTorch. It focuses on building models used in Medical area. In
        the project I was using this package to handle augmentations of the
        dataset as it provides first class support for 3D data.} % cite https://github.com/Project-MONAI/MONAI
    \item{matplotlib}
\end{itemize}

\section{Use case modelling and expected usage}
Here describe how to call the run.sh
We identified two particular use cases for the application, it should provide a
reasonably easy interface for training and then an interface for evaluating the
models given some training data. For this purpose a single Bash script called
wasc created which serves the 

\section{Loss and metrics implementation}

\section{SE implementation}

\section{NN implementation}



\chapter{Evaluation}
how we evaluate the networks
overlaps
dice / Jaccard
mapping

\section{Metrics}  % todo: prob. make this not a section
For evaluating the resulting predictions we decided to use Dice scores as they
are comminly used in many biomedical papers [TODO citation needed] but the end
user is not interested in the quality of the scores on cutouts around tunnels,
instead we must somehow use our neural network for predicting over the whole
images.
% [TODO more citations? espc. for Dice].


\section{Running the network}


\subsection{Mapping}
\label{section:mapping}
It is obviously important to calculate Dice and Jaccard on the whole images for
the final evaluations, but is it sometimes quite important to also see how well
the network segments tunnels is correctly identifies. For this purpose we
developed the following extra metrics.


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\linewidth]{resources/inkscape/matching_simple.png}
        \caption{}
        \label{fig:matchingsimple}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \includegraphics[width=\linewidth]{resources/inkscape/matching_multiple.png}
        \caption{}
        \label{fig:matchingmultiple}
    \end{subfigure}
    \caption{An example of three vertical pillars (black circles) and tunnels
    (in black)
    connecting them. In Subfigure \ref{fig:matchingsimple} we can see one good
    prediction and one false one. In SubFigure \ref{fig:matchingmultiple} we can
    see a single large prediction covering both tunnels.}
    \label{fig:matchingdiagram}
\end{figure}

The way we have defined speccialy look at tunnels which were correctly
identified and see how well the network worked in segmenting them out. In Figure
\ref{fig:matchingsimple} we can see a simple hypotetical schematic of a situation in which a
we have three vertical pillars, shown here as three black circles, and two
tunnels connecting them, here the tunnels are in black. Now the neural network
might predict multiple connected components, but only some of them are right
matches and correspond to a tunnel. In this figure, it is okay to say that the
blue tunnel is predicting one of the true tunnels, albeit it is little bit
oversegmenting the tunnel. On the other hand, the red predition is completely
off and it is not connected to any underlying tunnel; therefore, this should be
counted as a false predictions.

As we look into the data we might see that often time the tunnels, especially in
places where there a cluster of tunnels might not be perfectly sepparated. They
may be touching or even crossing each other. In some cases two tunnels visually
look like they have joined together. In this cases if we picked the strategy in
\ref{fig:matchingsimple} where predictions and labels are matched according to
the closest match, we would severally penalise predictions which touch. This is
a problem as it is weird to penalise the network if single predicted component
covers multiple predictions. The problem is again schematically visualised in
Figure \ref{fig:matchingmultiple}, where a a tunnel prediction in blue, matches
two different tunnels in black). I would argue that it is unfair to penalise the
metrics in this case.

This idea is techinally implemented in the following manner inside the
evaluations. We take the predictions of the neural networks and calculate the
component components in 3D. As described previously, the components are filtered
by size and thresholded. Now comes the matching. In the first pass the
application will iterate over all labels in the ground truth and see if there
are any predictions which contain at least 50\% of the mask. All of these
matches are then accumulated. In the implementation and further diagrams we
refer to this as all matches. The value 50\% was chosen so that a single ground
truth label cannot be contained in multiple predicted connected components.

Now that we have assigned possible multiple labels to each predicted connected
component we can divide the mapping into what is refered to as clean matches and
others. In Figure \ref{fig:bipartitematching} we see a diagram of an example of
a possible matching. In these case the network predicted three tunnels, and the
data has three labeled tunnels. In this case a predicted tunnel denoted as $p_1$
is uniquely mapped to label $l_1$. Since $p_1$ is not connected to any other
label, we call this a clean match. On the other hand predicted connected
component $p_2$ has according to the aforementioned algorithm been mapped to two
different labels $l_2$ and $l_3$. As previously argued it would be unwise to
disregard this mapping as a bad one, therefore we will also accept this mapping
and it will show up in following metrics when we refer to all matches.

Finnaly, this allows us to quantify how well different networks understand the
image not on a pixel by pixel manner, but in terms of the whole tunnels. We can
look at the prediction $p_3$ as a false positive, since it is not mapped to any
tunnel, while the label is $l_4$ is viewed as a missed tunnel, and therefore
called a false negative. All clean matches ($p_1$->$l_1$) are considered to be a
true positives, while multiple matches are, according to the precious arguments,
also considered as a true positive, in fact this is counted as a single true
positive.

To summaries, we are goind to look at the Jaccard and Dice score for the
following cases:
\begin{itemize}
    \item{Test: In this case we are looking at performance on cutouts of tunnels
        in one particular quadrant}
    \item{Evalution: In this case we are calculating the performance from the
        overal stitched binary images.}
    \item{Postprocesed overall: In this metric, the postprocessing described in Section
        \ref{sec:postprocessing} is applied on top of the predictions, and whole
        images are evaluated}
    \item{Postprocessed Clean matches: In this metric, we take just the clean
        matches and evaluate the binary masks.}
    \item{Postprocessed all matches: Like above, but even predictions mapped to
        multiple labels are used.}
    \item{Tunnel metric: Here we care about just the mapping described in Figure
        \ref{fig:bipartitematching}}
\end{itemize}
% todo: definitely deserves more attention
% todo: as an example, show the quad with all sort of tunnels connected together
% todo : btw am i explainign the how we binarise the labels?

\begin{figure}
    \begin{center}
        \includegraphics[width=0.4\linewidth]{resources/inkscape/matching_bipartite.png}
    \end{center}
    \caption{A diagram showing how multiple }.
    \label{fig:bipartitematching}
\end{figure}

Lastly, to reiterate the idea preseneted in the training Section [TODO: not
written yet], we are always training the network on three of the four possible
quadrants. As such, four different models are always trained and the results
will averaged.

\section{Ablation studies}
In this section we will present experimentally infered sizes of hyperparameters
for the neural network.

\subsection{Choice of depth and overlap for the network}
\label{sec:ablation-depth}
While working on the network the choice of depth and overlap is not obvious, but
it is a very important parameter as every advanced architecture is build on top
of the anisotropic unet described in Chapter [TODO], which has customizeable
depth. Moreover, the reason why the anisotropic version was created is to allow
us to change the depth.

In general it is accepted that the ability to teach deeper network helped improve
neural networks performance and in some sense, the ability to teach deeper
networks lead us to the neural network revolution. [TODO cit. needed] % TODO citation needed
But, the size of the network should be somehow connected to the
complexity of the problem, both in terms of size of the context and the image
complexity. To find a reasoanble value for this parameters we decided to do a 2D
hyperparameter search study. 

We took the 3D version of the Anisotropic architecture (AnisoUNet3D) and
considered five different versions. The shallowest one has exactly two
downsampling layers --- so it is referenced as a network with depth equal to 2
--- and the deepest one has 6 downsampling layers. Six downsampling layers was
chosen as this is enough to reduce the original size 7x64x64 to 7x1x1 as such,
there is no need to have a deeper network.

For the overlap hyperparameter we chose the values 0px, 10px, 20px, 30px, and
40px. The value 40px was chosen arbitrarily, as in this case, the neighbouring
cells already share more than half of the volume.

As the overlap is a hyperparameter of the postprocessing, this means that only
five different architectures must be trained to perform this analysis. Since we
are doing cross fold analysis, we are training 20 models. Finally, since neural
networks employ randomness, each combination of fold and architecutre is
repeated three times, leading to training 60 models for this evaluation.

The Figure \ref{fig:parametersearchstudy} showcases the dice score of the
various networks. Every number is the mean performance accross the 4-cross fold
validation and 3 indepedendant runs i.e. an average of 12 networks. Overall the
highest number is highlighted and it corresponds to the version with three
downsampling layers and 10px overlap; thus, all following models will be using
this combination of hyperparameters.
% TODO: tripple check, i had very different numbers before

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\linewidth]{resources/evaluations/eval_dice_mean_heatmap.png}
    \end{center}
    \caption{Evaluation Dice score for various depths of the network. Best
    values is highlighted.}
    \label{fig:parametersearchstudy}
\end{figure}


What we can also see from this hyperparameter search is that the effect of
number of downsampling layers is rather marginal and there is not a clear trend
of more downsampling layers leading to better performance or vice-versa. Another
effect that is visible is that while changing between 10 pixels to 20 or 30
pixels does minimal change in the performance, the difference between 0 pixels
and all others is visible in every case. Also it is interesting to see, that
altough the changes are not greatly different, the network with three
downsampling laters seems to always beat the others.

For further studies, we took a look at just 10 pixel overlap and looked at other
metrics, which were introduced in the previous chapters. In Figure
\ref{fig:eval:depth} the effect of the depth is studied more. I chose three
different metrics to investigate the effect of postprocessing and the
performance on the level of tunnels. Just to make clear, in these graphs, each
average is calculated as the mean of the performance accross four cross folds
and three runs. The error bars are +- a single sample standard deviation of
these values.

First of all, it is visible, that the performance differences accross various
depths is not that significant, but overall the network with three downsampling
steps and the network with five downsampling steps are achieving consistently
the highest averages, while the networks with two and six downsampling steps are
performing the worst. I believe this can be explained by the fact, that the
deepest network is unreasoanble large for our given problem, while the smallest
network might suffer from not enough layers.

However, it must be noted, that the error bars are very large and complicate any
conclussion, as it shows that the performance varies quite dramatically. This
lead us to investigate what could be causing this variety, and one possible
explanation is the various performance on the quadrants.

Taking a look at Figure \ref{fig:eval_quads} here we can see the same metrics,
but this time it is further divided among the various quadrants. What is
immediately obvious us that the error bars are less varied, due to the
performance differences accross quadrants. Interestingly, in some cases like the
models with 6 downsapling layers for the first quadrant has large performance
differences, but no reason as to why this happens was found. Since in this case
all bars are averages of three numbers, it is plausible that due to bad luck,
sometimes the network does not achieve as good of a performance as in other
runs. % TODO talk in discussion

In general, all models performed the best at the first quadrant (top left) and
third quadrant (bottom left), while the performance was lower on the second
quadrant (top right) and the fourth quadrant (bottom right). Looking at the
means and error bars now, it seems that the model with three downsampling layers
performs best or close to best in all folds, while having more layers can
sometimes reach similar performance it might also lead to worse (like in case of
three downsampling steps in quadrant 3); hence, we cannot say that for our input
size, the increased depth helps dramatically. But our previous decision to
choose the network of depth three in the following studies still stands, as even
tough the performance difference is not that different, this network is
consitently beating the smaller network and at the same time has much less
parameters than the larger networks.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/evaluations/eval_dice_mean_architecture_comparison.png}
        \caption{Evaluation performance.}
        \label{}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/evaluations/postprocess_overall_dice_architecture_comparison.png}
        \caption{Performance with post-processing.}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/evaluations/tunnel_dice_architecture_comparison.png}
        \caption{Performance on tunnels.}
        \label{fig:eval-depth-tunnels}
    \end{subfigure}
    \caption{Performance evaluations of the neural networks for various depths
    and fixed 10 pixel overlap.}
    \label{fig:eval:depth}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/evaluations/eval_dice_mean_by_quad_arch.png}
        \caption{Evaluation performance.}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/evaluations/postprocess_overall_dice_by_quad_arch.png}
        \caption{Performance with post-processing.}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/evaluations/tunnel_dice_by_quad_arch.png}
        \caption{Performance on tunnels.}
        \label{subfig:eval_tunnels}
    \end{subfigure}
    \caption{Performance evaluations of the neural networks for various depths
    and fixed 10 pixel overlap.}
    \label{fig:eval_quads}
\end{figure}

\subsection*{Different architectures}
As we have discussed in the previous section, we did a 2D hyperparameter search
to choose a good estimate of the depth and overlap values, since these are used
by our advance neural networks, we picked 3 downsampling layers and an overlap
of 10 pixels. In this section the evaluation of other networks will be
presented.

We decided to study the following architectures: a basic implementation of UNet
as described in Section [TODO]. Its anisotropic version as described in Section
[TODO], a version of Anisotropic model with the CSAM module as described in
Section \ref{sec-csammodule}, and finally the UseNet architecture from Section
\ref{sec-seblock}. All models are using 10 pixel overlap and their depth is
equal to three downsampling blocks, with the exception of BasicUNet, which has
two downsampling blocks. Lastly, what is referred to as a 2D and 3D version of
most complex models were created. In the 3D versions all convolution blocks,
which are technically refered to as Horizontal blocks, are using 3x3x3 sized
kernels to perform all convolutions, while the 2D versions are using 1x3x3 sized
kernels. Note that the first dimension is equal to the depth or the z slices.

The idea to test the difference in their performance came from the following two
concepts: when I as a human am looking at the images, I can roughly estimate
what are tunnels and what is a pillar based on the overall shape, but it is
significantly easier if I am allowed to see multiple slices; hence, the 3D
version with their greater receptive fields reaching other z slices, would
benefit in terms of their accuracy. On the other hand, network with 3D kernels
have more trainable parameters; thus, might be harder to train and as we have
sparsely annotated dataset this could become a hurdle causing the 2D version to
perform better.

Moving on to the results Figures \ref{fig:eval-diff-arch-dice}-
\ref{fig:eval-diff-arch-tunnel} shows the same metrics we discussed in Section
\ref{sec:ablation-depth} aggregated accross all the cross folds, while
\ref{fig:eval-diff-arch-dice-quads}-\ref{fig:eval-diff-arch-tunnel-quads} shows
the performace for each quadrant. The means and standard derivations of all
metrics was calculated exactly the same as in the previous section, again every
combination was trained three times.

We can again see a similar pattern with the large error bars in all figures
especially the Figure \ref{fig:eval-diff-arch-tunnel} when looking at the tunnel
metrics. Of note is that the BasicUNet architecture is performing worse than all
others in every metric even accounting for the standard derivation.

As such, the problem of large distrepancy in perfoamnce is similarly caused by
different performance accross the folds, where the overall dice performance is
highest for the third quadrant while being relativaly lower for the second
quadrant and fourth quadrant in Figure
\ref{fig:eval-diff-arch-dice-overall-quads}.

While it is difficult to asses the overall trends due to the error bars, an
interesting observation is that the 2D versions tend to perform equally like in
third quadrant in Figure \ref{fig:eval-diff-arch-dice-overall-quads} or slighly
better than their 3D versions. I suspect this could be caused by having
significantly less parameters compared to the 3D versions.

Since the error bars are quite large even tough the value is calculated from 12
values for each bar, it would be insightful to find out what causes this.
Inspired by the previous hyperparameter search section, additional graphs, where
the performance is analysed per quadrant were created in Figures
\ref{eval-diff-arch-dice-quads}-\ref{eval-diff-arch-tunnel-quads}.

In Figure \ref{eval-diff-arch-dice-quads} the overall evaluation dice is shown
and in Figure \ref{eval-diff-arch-dice-overall-quads} are results with the
postprocessing applied. Looking at the dice results it is clear that the
postprocessing neealy always helps increase the metric's value, but it is rather
on the second decimal position. And the effect is largest on the third quadrant.

If we were to just focus on the nicely segmented tunnels i.e. those that were
matched we can see that the overal dice score jumps to higher values, reaching
even around 0.68 in the second quadrant. Hence, it looks like the overall Dice
score is being lowered by false predictions, which even the postprocessing is
not able to remove.

Lastly, we will talk about the Dice score on the level of tunnel detection as
was described in Section \ref{sec:matching}. Compared to the overall Tunnel
scores in Figure \ref{fig:eval-diff-arch-tunnels}, where it seemed according to
the average values, that the 2D version have slight advantage, here it is not so
clear, there also seems not be any clear clearly better architecture than the
rest.


\begin{figure}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{resources/evaluations/diffarchs/eval_dice_mean_architecture_comparison.png}
    \end{center}
    \caption{}
    \label{fig:eval-diff-arch-dice}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{resources/evaluations/diffarchs/postprocess_overall_dice_architecture_comparison.png}
    \end{center}
    \caption{}
    \label{}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{resources/evaluations/diffarchs/postprocess_clean_matched_dice_architecture_comparison.png}
    \end{center}
    \caption{}
    \label{}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{resources/evaluations/diffarchs/postprocess_matched_dice_architecture_comparison.png}
    \end{center}
    \caption{}
    \label{}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{resources/evaluations/diffarchs/tunnel_dice_architecture_comparison.png}
    \end{center}
    \caption{}
    \label{fig:eval-diff-arch-tunnel}
\end{figure}


\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{resources/evaluations/diffarchs/eval_dice_mean_by_quad_arch.png}
    \end{center}
    \caption{eval dice}
    \label{fig:eval-diff-arch-dice-quads}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{resources/evaluations/diffarchs/postprocess_overall_dice_by_quad_arch.png}
    \end{center}
    \caption{overall dice}
    \label{fig:eval-diff-arch-dice-overall-quads}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=\linewidth]{resources/evaluations/diffarchs/postprocess_clean_matched_dice_by_quad_arch.png}
    \end{center}
    \caption{clean matched}
    \label{fig:eval_diff_arch_clean_match_dice_quads}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\linewidth]{resources/evaluations/diffarchs/postprocess_matched_dice_by_quad_arch.png}
    \end{center}
    \caption{matched dice}
    \label{fig:eval_diff_arch_matched_dice_quads}
\end{figure}
\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\linewidth]{resources/evaluations/diffarchs/tunnel_dice_by_quad_arch.png}
    \end{center}
    \caption{tunnels}
    \label{fig:eval-diff-arch-tunnel-quads}
\end{figure}

\section{Recommended values}
As was discussed in the previous sections using the hyperparameter study and 

\chapter{Discussion}
what to do next

\end{document}
