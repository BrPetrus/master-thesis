%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital,     %% The `digital` option enables the default options for the
               %% digital version of a document. Replace with `printed`
               %% to enable the default options for the printed version
               %% of a document.
%%  color,       %% Uncomment these lines (by removing the %% at the
%%               %% beginning) to use color in the printed version of your
%%               %% document
  oneside,     %% The `oneside` option enables one-sided typesetting,
               %% which is preferred if you are only going to submit a
               %% digital version of your thesis. Replace with `twoside`
               %% for double-sided typesetting if you are planning to
               %% also print your thesis. For double-sided typesetting,
               %% use at least 120 g/m² paper to prevent show-through.
  nosansbold,  %% The `nosansbold` option prevents the use of the
               %% sans-serif type face for bold text. Replace with
               %% `sansbold` to use sans-serif type face for bold text.
  nocolorbold, %% The `nocolorbold` option disables the usage of the
               %% blue color for bold text, instead using black. Replace
               %% with `colorbold` to use blue for bold text.
  lof,         %% The `lof` option prints the List of Figures. Replace
               %% with `nolof` to hide the List of Figures.
  lot,         %% The `lot` option prints the List of Tables. Replace
               %% with `nolot` to hide the List of Tables.
]{fithesis4}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = mgr,
    department  = Department of Visual Computing,
    author      = Bruno Petrus,
    gender      = m,
    advisor     = {Assoc. Prof. Martin Maška, Ph.D.},
    title       = {Segmentation of Tunneling Nanotubes Using U-Net-Like Neural Networks},
    TeXtitle    = {Segmentation of Tunneling Nanotubes Using U-Net-Like Neural Networks},
    keywords    = {neural networks, segmentation, tunnelling nanotubes, U-Net architecture, fluorescence microscopy},
    TeXkeywords = {neural networks, segmentation, tunnelling nanotubes, U-Net architecture, fluorescence microscopy},
    abstract    = {%
        This thesis addresses the automated semantic segmentation of Tunnelling Nanotubes (TNTs) in 3D Drosophila wing microscopy images. The work tackled challenges posed by high spatial anisotropy and extremely sparse annotations.
        
        The primary contribution is the development of AnisoUNet, an anisotropic U-Net variant tailored for processing 3D data with restricted Z-axis resolution. The solution utilised a robust 4-fold cross-validation pipeline and a comprehensive inference system.
        
        The study demonstrates the feasibility of using deep learning for segmenting these nanoscale structures under severe data limitations. The resulting segmentation capability offers a basis for automating the quantitative analysis of cell morphogenesis dynamics.
    },
    thanks      = {%
        I am very grateful to my advisor, Assoc. Prof. RNDr. Martin Maška, Ph.D., for his sustained mentorship, constructive critiques, and ready availability throughout the whole time.

        I am also grateful to Prof. Osamu Shimmi (University of Tartu, Estonia) and Dr Ngan Vi Tran (University of Tartu, Estonia) for providing the dataset analysed in this thesis.

        I want to thank my friends for providing me with a great deal of encouragement. My deepest gratitude goes to my partner, Tereza Zuskinová, who has offered me unconditional support that carried me through the most challenging part of this research.
    },
    bib         = biblio.bib,
    %% Remove the following line to use the JVS 2018 faculty logo.
    facultyLogo = fithesis-fi,
    %declaration = {test}
    declaration = {
        Hereby I declare that this paper is my original authorial work, which
        I have worked out on my own. All sources, references, and literature
        used or excerpted during elaboration of this work are properly cited
        and listed in complete reference to the due source.
        

        \noindent During the preparation of the thesis, I used the following AI tools:
        \begin{itemize}
            \item{Grammarly for grammar check.}
            \item{GitHub Copilot for faster code development, writing documentation, and code consultations.}
            \item{ChatGPT to improve my writing style.}
        \end{itemize}
        I declare that I used these tools in accordance with the principles of academic integrity. I checked the content and took full responsibility for it. 
    }
}
\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{listings} %% Source code highlighting
\usepackage{subcaption}
\usepackage[style=numeric]{biblatex}  %% use square brackets for citations
\subcaptionsetup{font=small}      % For subcaptions
\usepackage{overpic}  %% Place text over the images

\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
\usepackage[babel]{csquotes} %% Context-sensitive quotation marks

\usepackage[ruled]{algorithm2e}  %% Algorithms

\begin{document}
%% The \chapter* command can be used to produce unnumbered chapters:
\chapter*{Introduction}
%% Unlike \chapter, \chapter* does not update the headings and does not
%% enter the chapter to the table of contents. I we want correct
%% headings and a table of contents entry, we must add them manually:
\markright{\textsc{Introduction}}
\addcontentsline{toc}{chapter}{Introduction}

Semantic segmentation is a critical task in computer vision that involves assigning a specific class label to every image element in a given image. This thesis focuses on the segmentation of tunnelling nanotubes (TNTs), which are cell membrane protrusions observed during cell morphogenesis in Drosophila wings. While the exact function of these structures remains under investigation, their image-based analysis is currently limited by the difficulty of manually annotating them in three-dimensional space.

The dataset for this research, provided by Prof Osamu Shimmi from the University of Tartu, captures a cellular structure known as the Interplanar Amida Network (IPAN). This network consists of vertical "pillars" connected by horizontal TNTs, captured using fluorescence microscopy. The image data presents significant challenges for automated analysis: it is heavily anisotropic, and while the dataset spans twenty time slots, reference annotations were provided for only two time steps. As such, the dataset is sparsely annotated.

To address these challenges, we adapted the U-Net architecture, a standard for biomedical segmentation, to better handle the specific constraints of our image data. An anisotropic version of a U-Net-like architecture called AnisoUNet, which utilises anisotropic kernels, is introduced. Additionally, we investigate an attention mechanism to improve the network's performance. The performance of our network is extensively evaluated using a 4-fold cross-validation, and its design choices are justified using an ablation study.

Chapter 1 provides an overview of the biological context and image data preprocessing. Chapter 2 shows related work on TNT segmentation. Chapter 3 describes the implemented neural network architectures, including the AnisoUNet and attention variants. Chapter 4 outlines the training methodology, including data splitting and loss functions. Chapter 5 covers the technical implementation and software environment. Chapter 6 evaluates the models, presenting an ablation study on hyperparameters and comparative benchmarks. Finally, Chapter 7 concludes the thesis.


\chapter{Data}
\label{sec:data}
% todo: probably add some more examples
During cell morphogenesis, growth of protrusions has been observed using fluorescent microscopy. Their effect on the growth is still not fully understood, but has sparked multiple theories. Some researchers speculate these tunnels are used for long-range signalling; they may be part of pathological conditions, where viruses use them [TODO CITE] [Add more Examples][TODO move to the Introduction chapter].

Our partner, Dr Osamu Shimmi from the Institute of Molecular and Cell Biology at the University of Tartu in Estonia, was involved in studying one of such places where these protrusions are present - the Drosophila wings - and the findings are available in Paper \parencite{Tran2024Programmed}. They have used in vivo imaging to study the cellular structures.
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.4\textwidth]{resources/data/wings.png}
    \end{center}
    \caption{A schematic of the structure of the Drosophila wings. Two nearly identical layers of epithelium cells are supported by microtubule protrusions, which are vertical and referred to as pillows in this work. There are also horizontal protrusions connecting the pillows in nearly horizontal direction - the so-called tunnelling nanotubes or TNTs.}
    \label{fig:wingsdiagram}
\end{figure}

During the development of the larva, specifically in the pre-pupal stage, the pupal wing is formed. Afterwards, the single-layer wing separates into two individual layers of epithelial cells, as shown in Figure \ref{fig:wingsdiagram}. The vertical protrusions connecting the layers will be referred to as pillars, whereas the vertical microtubular protrusions will be called tunnelling nanotubes (TNTs).

In Paper \parencite{Tran2024Programmed}, the authors refer to the overall structure as Interplanar Amida Network (IPAN). They note that IPAN is dynamic and can change significantly in just half an hour, during which the TNTs may connect with different vertical pillars or completely disappear, while some TNTs exhibit stability. While the authors delve into greater depth regarding what exactly happens during the creation and disappearance of these structures, they are limited to a qualitative investigation. The goal of this thesis was to explore U-Net-like models for segmenting tunnelling nanotubes in these 3D images, thereby helping researchers analyse properties such as the average duration of these tunnels. 

Dr Shimmi provided us with multiple datasets, but we obtained sparse annotations for only one of them; hence, we will focus on this one. In Figure \ref{fig:dataexamples}, a single slice on the left and a volume visualisation of the data on the right are shown. The whole dataset comprises twenty time slots and was captured by using a single fluorescent dye. Each time snapshot consists of seven 512x512 grayscale, 16-bit images. Due to the work detailed in \parencite{Tran2024Programmed} the particular dye and strain of the Drosophila fly allows us to see the membranes of both horizontal tunnelling nanotubes and vertical pillars in Figure \ref{fig:dataexamples}. Each of the slices represents a region of $56.32 \times 56.32 \mu m$ and the slices are $1\mu m$ apart, meaning that each voxel is about $1 \times 0.11 \times 0.11 \mu m$ in size. Since the dye is activated in cell membranes, the pillars in our data appear hollow. 

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.42\textwidth}
        \includegraphics[width=\textwidth]{resources/t017z0_saturated1percent.png}
        \label{fig:dataexamplesslice}
    \end{subfigure}\hfill
    \begin{subfigure}{0.55\textwidth}
        \includegraphics[width=\textwidth]{resources/data/volume_viewer_modified.png}
        \label{fig:dataexamlesvolume}
    \end{subfigure}
    \caption{Left: a single depth slice of the data at time 17 is shown. Right: a volume visualisation of the IPAN strudata from different time slots. Both images were contrast-enhanced for visualisation purposes.}
    \label{fig:dataexamples}
\end{figure}

\section*{Labelling and preprocessing}

The biologist created a protocol which they followed by annotating the samples. They decided to preprocess the data by converting the 16-bit grayscale images into 8-bit grayscale images using a 1\% percentile stretch operation.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/mask000-slice3-composite-circles.png}
        \label{fig:gt-slice-3-composition}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{resources/mask000-slice3-saturated.png}
        \label{fig:gt-slice-3-saturated}
    \end{subfigure}\hfill
    \caption{Single slice of the data on the left (contrast-enhanced) with overlaid tunnel labels in magenta. Two yellow circles emphasise two places with potentially missing labels. On the right, the instance labels for that particular slice.}
    \label{fig:gt-slice-3}
\end{figure}

After preprocessing is done, the protocol states that, by picking random time slots, at least 100 instances of tunnels must be annotated, and each frame must be fully annotated. Also, segments originating from the surrounding tissue should be ignored. According to this protocol, the biologists picked two time steps — the 0th and 17th steps — to label. The reference annotations for a single slice of the 0th time slot are visible in the right part of Figure \ref{fig:gt-slice-3}. Overall, in the two time steps, 127 tunnels are fully annotated.

Sometimes it is rather challenging to decide if the protrusion is a TNT or not. In Figure {fig:gt-slice-3-composition}, two protrusion-like structures are highlighted using yellow circles. Although their structure appears essentially the same as that of a TNT, it is unclear why the biologist did not also label them. This illustrates a possible difficulty in assessing the accuracy of the segmentation.

Afterwards, the labels were manually checked, and two cases of label duplication were identified and corrected by assigning a unique number to the new connected component. The corrected reference annotations will be published in the archive.

\chapter{Related Work}

\label{sec:research}
%TODO: add def. of segmentation task to the intro

We began by researching papers explicitly dealing with TNTs. I found one that used a non-machine learning approach \parencite{Hodneland2006Automated} and another that employed a machine-learning approach with a U-Net architecture \parencite{Ceran2022TNTdetect}. Later, we examine solutions related to the topic of curvilinear structure segmentation \parencite{Mou2021}, and finally explore generic improvements utilising an attention mechanism in \parencite{Rundo2019}.

The afore-mentioned U-Net architecture was first described in a seminal paper by Ronneberger et al. in 2015 \parencite{Ronneberger2015}. The authors describe the architecture and use it for the semantic segmentation of biomedical images. The authors achieved impressive results with a minimal dataset of about 30 images. Since then, the architecture has become a standard method for solving segmentation tasks in various image modalities; thus, we have also adopted this approach. A more detailed explanation of the architecture can be found in Section \ref{sec:unet}.

% todo: check if they are really looking at the same thing as us

The first paper that explicitly mentions the segmentation of TNTs is by Hodneland et al. \parencite{Hodneland2006Automated}, who are also examining protrusions connecting cells. However, in their case, the tunnels form long, straight lines, unlike our TNTs, which are not always straight. The authors describe a multistep algorithm that employs two channels — one for staining cell membranes and TNTs, and the other for staining cytoplasmic compartments — to segment the TNTs. Since our data have different modalities, this approach cannot be used.

The other paper, which mentions explicitly tunnelling nanotubes, is \parencite{Ceran2022TNTdetect}. In this paper, the authors develop a tool for segmenting TNTs called TNT.AI. They discuss how to address the challenge of manually marking the tunnels. Four human experts were involved to correctly identify TNTs (as individual experts' labels varied), and measures were taken to ensure the labels were as clear as possible. Then they devised a two-step procedure. In the first step, a classification network works on tiles to identify where TNT is possible, and then a second network is used to finely segment the TNTs pixel-wise. Unfortunately, they do not go into details about their networks. I tried to contact one of the authors to get the code and data, but they did not respond. In the paper, they discuss using a U-Net-like architecture for the segmentation part, which is composed of ResNet blocks. 

As neither of these papers provides working code examples or is applicable to our data, we decided to look at the tunnels as an example of curvilinear structure. Bibiloni et al. \parencite{bibiloni2016curvilinear} provide a complete definition of curvilinear structures based on their geometric and photometric characteristics. However, they also provide a simpler, rough definition, which is enough for our purposes. They can be defined as thin, long, line-like regions with different intensities than their neighbouring pixels \parencite{bibiloni2016curvilinear}. Some examples of curvilinear structures include river finding, blood vessel segmentation, or lung airways segmentation \parencite{kong2018roadsensing} \parencite{Mou2021}. Our TNTs mostly fit this description as their shape is thinner compared to the vertical protrusion, and they are clearly different in intensity from the background.

Mou et al. \parencite{Mou2021} use a modified U-Net to segment curvilinear structures such as blood vessels in the retina. They investigate the use of a typical U-Net and introduce the so-called spatial and channel attention module (SCAM), which is purported to help maintain the connection of faint curvilinear structures.

Given all this information, we decided to adapt the standard U-Net architecture for our 3D images. We will discuss how we addressed the issue of anisotropy in our data and how we drew inspiration from \parencite{Mou2021} and \parencite{Ceran2022TNTdetect} to implement an attention mechanism to enhance accuracy.

\chapter{Architectures}
In this chapter, the various architectures that were used are described.

\section{U-Net and BasicUNet}
\label{sec:unet}

Chapter \ref{sec:research} introduces the U-Net architecture in conjunction with its uses in related works. The main idea was as follows: in other tasks, such as image classification, we typically have an encoder followed by a neck, which culminates in a classification head. The authors speculated on how to extend this mechanism to achieve pixel-wise classification, or in other words, semantic segmentation. They propose placing several upsampling layers after the encoder part to upsample the extracted features.

The overall diagram is shown in Figure \ref{fig:unetdiagram}. All convolutional layers use a $3 \times 3$ kernel followed by a ReLU operation, while the downsample and upsample operations use $2 \times 2$ kernels.

The architecture consists of four main parts: the encoder, the bottleneck, the decoder, and a final convolution to reduce the number of channels. In the encoder part, two convolutional layers are followed by a downsample operation, and the whole workflow is repeated four times. Notice that the convolutions double the number of channels. After the fourth downsampling operation comes two convolution operations, called the bottleneck. The decoder follows; the features are upsampled, halving the number of channels and doubling the spatial dimensions, and concatenated with features from the corresponding layer in the encoder part. This is repeated four times. Finally, the rightmost convolution operation is responsible for reducing the number of channels into the corresponding number of classes for the final output segmentation map. Notice that the output segmentation mask has a different size from the original image.

The presented model works with 2D data, but our data is 3D. Fortunately, the modification is not difficult; all convolutions can be replaced with their 3D equivalent. Moreover, we wanted to obtain an output size equal to the input data; therefore, padding was used in the convolutions. Downsample operations were implemented using a 3D max-pool operation with a $2 \times 2 \times 2$ kernel, and upsampling is technically implemented with transposed convolutions using a $2 \times 2 \times 2$ kernel shape. Finally, as our task is a binary segmentation class, our output has a single channel encoding both classes, as 1.0 corresponds to a tunnel and 0.0 corresponds to anything else. Due to the nature of halving all spatial dimensions isotropically in the downsampling layers, after just two downsamples, the z dimensions are reduced from 7 to 1 slices; thus, our model has two downsampling layers. This implementation is referred to as BasicUNet, which is our first model.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{resources/architecture_diagrams/unet.png}
    \end{center}
    \caption{U-Net architecture as described in \parencite{Ronneberger2015}. In the left part of the network is the encoder formed of four downsampling operations followed by a convolution block. At the bottom is a convolution block called a bottleneck, and finally, four upsampling layers are used, along with convolution blocks, to upsample the features. The rightmost convolution creates the final segmentation map with the correct number of classes. The horizontal lines indicate how the data is concatenated across relevant layers. Taken from \parencite{Ronneberger2015}.}
    \label{fig:unetdiagram}
\end{figure}

\section{Anisotropic U-Net (AnisoUNet)}
\label{sec:anisounet}

Papers such as the influential VGGNet \parencite{simonyan2015vgg} demonstrated that model depth is one of the critical hyperparameters of a network that can be optimised to improve its accuracy. This inspired us to create the following architecture.

Since our data is heavily anisotropic, we are limited in how much we can downsample in the z axis, as we have only seven voxels inside the z dimension, meaning we can downsample just two times. This presents an issue, as we cannot experiment with a deeper network. Moreover, after just two downsampling steps, the features' z dimensions are reduced to one layer, which we believe means that the network has to quickly compress all relevant information, potentially inhibiting the expressive performance of the network.

Our solution was to introduce anisotropic kernels into the design. Instead of using the isotropic $2 \times 2 \times 2$ kernel shapes for both downsampling and upsampling, $1 \times 2 \times 2$ kernel shapes are used to keep the z dimension constant. As such, this allows us to train deeper networks and does not force the network to compress all depth information like before.

There is still the question of what kernel size to use for the horizontal layers. In the Section \ref{sec:diff-architectures} we will study this choice in more detail. We will define the 3D and 2D versions of the network, by which we mean that 3D networks use $3 \times 3 \times 3$ kernel shapes in their convolutional layers, while 2D networks use $1 \times 3 \times 3$ kernel shapes.

The idea to test the difference in their performance came from the following two concepts: when a human is looking at the images, they can roughly estimate what are tunnels and what is a pillar based on the overall shape, but it is significantly easier if they are allowed to see multiple slices; hence, the 3D version with their greater receptive fields reaching other z slices, would benefit in terms of their accuracy. On the other hand, networks with 3D kernels have more trainable parameters; thus, they might be harder to train, and as we have a sparsely annotated dataset, this could become a hurdle, causing the 2D version to perform better.

Overall, such a network with anisotropic pooling operations will be referred to as AnisoUNet(3D) throughout the remainder of this thesis.

\section{The CSAM module}
\label{sec-csammodule}

This section discusses the channel and spatial attention module (hereafter referred to as CSAM), as mentioned in the CS-Net paper \parencite{Mou2021}. 

\begin{figure}[htbp]
   \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{resources/architecture_diagrams/csam.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{resources/architecture_diagrams/csam_descp_croped.png}
    \end{subfigure}
    \caption{ The CSAM. This module is inserted in the bottleneck part of an U-Net neural network. N is equal to $D \times W \times H$, and C stands for the number of channels. F is the input features of size $N \times C$. In the top part is SAM, and CAB is at the bottom. Q, K, J, and V are features created by applying the specific convolutions. Adapted from \parencite{Mou2021}}
    %todo: check if my implementation is correctly doing this
    \label{fig:csammodule}
\end{figure}

Figure \ref{fig:csammodule} shows the overall architecture of the CSAM. It is formed of two paths, the spatial attention block SAB and the channel attention block CAB. The former is designed to help the network identify tubular structures along the spatial direction, whereas the latter is used to help the network learn inter-channel dependencies. The following sections will discuss the main principles behind the blocks, but for a full explanation, please refer to the original paper. Note that the authors devised both 2D and 3D versions of this module, but we will focus on just the 3D version.

\subsection*{SAB}
The design of the spatial attention block is motivated by the detection of tubular structures, such as blood vessels, in retinal images. The detection of such features might require contextual information rather than purely local information.

Overall, the design is very similar to the attention mechanism in Transformers described by Vaswani et al. \parencite{Vaswani2017attention} where a key-query-value structure was introduced. In the case of CSAM, the query Q, key K, and newly introduced judge J matrices are created by convolving the data with $1 \times 3 \times 1$, $3 \times 1 \times 1$, and $1 \times 1 \times 3$ kernel shapes. This step compresses the information about the tree-like structures in the three directions. These matrices are then multiplied together to obtain a spatial attention matrix between the x, y, and z directions, called the Spatial Affinity Matrix. This attention map is then used to weight the dimension-reduced original data in matrix $V$ and expand it back to its original size.

\subsection*{CAB}
The design of the second block is very similar to the first one, but in this case the point is to model the interchannel dependencies, therefore instead of using the differently sized kernels, the authors use $1 \times 1 \times 1$ kernels, which results are then combined to an affinity matrix of size $C \times C \times C$, where a value at position $(x,y,z)$ tell how these three channels are connected.

Afterwards, the output is the sum of the SAB, CAB and the original data. This architecture is referred to as AnisoUNEt-CSAM in the thesis.

\section{Squeeze-and-excitation block}
\label{sec-seblock}

In \parencite{Hu2018}, researchers demonstrated how a simple small block, called the squeeze-and-excitation (SE) block, can improve the performance across various datasets and tasks. Hence, we chose it as the second attention-like mechanism to explore in this thesis.

The general principle of a squeeze-and-excitation block is illustrated in Figure \ref{fig:seblock}. The main idea is that we allow the network to learn the significance of each channel automatically during training. As its name suggests, there is a squeeze operation where the channels are compressed, followed by an excitation step that produces channel weights between 0.0 and 1.0, which multiply the values in the original data. The researchers postulate that this improves the network's quality of representation by modelling the interdependencies between channels of the output of the convolution layers \parencite{Hu2018}.

Let us look at it in more detail. The SE block, shown in Figure \ref{fig:seblock}, consists of four operations: truncate, squeeze, excitation, and scaling. The truncate operation, denoted as $F_{TR}$, is a general convolutional operator that reduces the dimension. This step is not crucial in the scheme, but it is mainly to illustrate the point that the SE block is usually built upon some operation. More importantly, the transformed data is then run through a squeeze operation, which performs a global operation - in our case, global average pooling - across the entire channel. After that, the excitation operation comes. This operation was designed to possess the following properties: they aimed to create a flexible operation, meaning one capable of learning non-linear dependencies, and they wanted it to be non-mutually exclusive across channels. These properties are fulfilled by applying two convolutions, followed by ReLU and sigmoid, respectively. Finally, the $F_{scale}$ uses the attention weights to scale the data in $U$. The combination of squeeze and excitation allows the network to learn how to weight each channel (the scale operation mentioned above) by considering multiple channels.

% todo: check if it is global average pooling
% todo: move to implementation
% Note that the first convolution produces output with less than the original number of channels and the ratio is called the reduction factor. 

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\linewidth]{resources/architecture_diagrams/seblock.png}
    \end{center}
    \caption{A squeeze-and-excitation block. The input data $X$ is processed to create $\hat X$. First, the data is truncated by a $F_{tr}$ operation to form features $U$. The rescale operation $F_scale$ rescales the channels according to their importance learnt in the excitation ($F_{ex}$, typically a convolutional layer) and squeeze operations ($F _ {sq} $, typically a global average pooling). Taken from \parencite{Hu2018}}
    \label{fig:seblock}
\end{figure}

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\linewidth]{resources/architecture_diagrams/usenet.png}
    \end{center}
    \caption{Placement of the SE blocks inside U-Net in the USE-Net architecture. Taken from \parencite{Rundo2019}.}
    \label{fig:usenet}
\end{figure}

While the authors in \parencite{Hu2018} talk about placing this object after the truncate operation, and give concrete examples of where to place it in Inception and ResNet blocks, the U-Net is not mentioned. We briefly experimented with placing a single SE block in the bottleneck part of the U-Net, similarly to how the CSAM was used; however, this was later replaced by using the architecture described in \parencite{Rundo2019}. In their work, they suggested a different placement of the SE blocks inside a U-Net-like architecture. It is called USE-Net, and the placement is visible in Figure \ref{fig:usenet}. The authors place an SE block before the concatenation and at every step of the upsampling. 

This is the last architecture we will explore in this work and will be referred to as the AnisoUNet-UseNet variant. 

\chapter{Training}
\label{sec:training}
This chapter explains how the aforementioned neural networks were trained. How the data was split to allow cross-fold learning, and finally, how the network can be run to segment images of different sizes.

\section{Splitting the data}
\label{sec:processing_data}
% [TODO mention alternatives to supervised learning somewhere]

As mentioned in Chapter \ref{sec:data}, we have only two fully annotated time frames, each of 7x512x512 voxels. Splitting such a small dataset into training and testing sets presents a challenge, which is further complicated by the need to experiment with fully 3D networks. If we were to treat each depth slice independently, it would allow mixing of testing and training data; however, this could introduce bias. This bias arises because the pillars are nearly vertical, meaning their position is roughly the same in all z-slices, and the tunnels are not limited to a single z-slice. Moreover, if we consistently use all seven depth slices, we would essentially be training on one time frame while testing on a different one. In our opinion, this approach is unfeasible.

However, as U-Nets have demonstrated, they do not require a vast labelled dataset to be trained. In the original paper, a few dozen images were used to train the networks to segment medical images \parencite{Ronneberger2015}. Similarly, we were able to reproduce the results of the CS-Net paper \parencite{Mou2021} and train a retina vessel segmentation network with just a few dozen images.

While splitting the dataset into quadrants would still yield only about eight images, we decided to look at the level of the annotated instances of tunnels. Since there are more than 100 of these TNTs, we can train a network on just the crops of the TNTs. Then we can tile the entire image and stitch the predictions of the neural networks similarly to what was done in the original U-Net paper \parencite{Ronneberger2015}.

The actual splitting logic iterates through every single TNT and crops it out. As the tunnels have different sizes, we decided to cut out the bounding boxes of the tunnels with a set of minimum sizes. For the minimum size, we opted for a $7 \times 90 \times 90$ shape, as it comfortably fits the majority of tunnels and provides a decent context around it. A decent number of tunnels are smaller than this. To expand the size to fit the minimum volume size, we decided to expand them with relevant data instead of padding by 0.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/training/t017_upperright_modified.png}
    \end{center}
    \caption{Part of a single slice of the data (contrast enhanced). Two yellow arrows point out noisy sections.}
    \label{fig:upperright}
\end{figure}

It is still important to be able to somehow split the dataset into the training and testing sets. As the dataset is rather sparse, choosing a random subset to evaluate performance is not ideal, and there is a random chance that some tunnels from the testing set may be visible in a cutout of a tunnel in the training set. Another problem is how the various parts of the dataset do not have the same number of tunnels, tunnels of the same morphology and the same amount of noise and irrelevant structures. For example, in the upper right corner in Figure \ref{fig:upperright} we can find some irrelevant structures which are not present in the rest of the dataset. Given these problems, it is pretty tricky to split the dataset in such a way that all possible types of artefacts and tunnel structures are visible in both the testing and training subsets.

We tackled this problem by doing a quadrant-based spatial 4-fold cross-validation. In each training run, one of the four quadrants is selected as the testing quadrant, and all cropped TNTs belonging to this quadrant form the testing subset. This helps mitigate the small dataset and enables us to study how the different folds affect the performance of the neural network.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.6\linewidth]{resources/dataset/vis_split_t0.png}
    \end{center}
    \caption{Maximum projection of the binarised reference annotations for the first fold. Red crops belong to crops which were placed into the testing set, while blue crops form the training set. Tunnels whose crops were significantly present in both training and testing quadrants were discarded; thus, the tunnels do not have any colour.}
    \label{fig:folding-visualisation}
\end{figure}

This is still quite challenging to do correctly, as not every tunnel lies neatly inside one of the quadrants. A split used in training is visualised in Figure \ref{fig:folding-visualisation}, where a possible split is shown in two colours: red represents crops belonging to the testing subset. The decision of whether a crop belongs to the testing subset or not is determined by examining the overlap with the training quadrant. If the crop lies with more than 20\% or has more than 50 voxels in the testing quadrant, it will not be included in the training subset. If at least 80\% of the crop is inside the testing quadrant, it is considered to be part of the testing subset.

Finally, during the training, we need to know when to stop. For this purpose, during each run, approximately 30% of the crops for training are randomly picked and form the validation set.

\section{Loss functions}
Since we can view finding tunnelling nanotubes as a binary classification problem applied at every pixel, it naturally comes to using a binary cross-entropy loss to train the network to segment the images. However, our dataset is heavily imbalanced, as it contains a significantly higher number of negative examples. To somewhat help the training pay more attention to the tunnels, the cross-entropy was weighted by a factor of 50, which was estimated as the ratio of pixels classified as background and pixels assigned to some tunnel by the biologist. The weighted BCE loss is defined as in \parencite{Jadon2020loss,PyTorchBCE}, where $y$ stands for prediction, $\hat{y}$ is the true label, and $beta$ is the weight:

\[ L_{BCE}(y, \hat{y}) = -(\beta y \log(\hat{y}) + (1-y)\log(1-\hat{y}) \]

To further improve the performance of the training, we drew inspiration from the CS-Net paper, which uses Dice loss. Some papers claim that the combination of Dice loss and cross-entropy loss leads to better performance in many segmentation tasks \parencite{ma2020segmentationlossodyssey}. This is defined as taking $L_{Dice} = 1 - \text{Dice}$, meaning that as we obtain segmentation results closer to the ideal mask, we approach 0. The loss function was defined using as in the paper \parencite{ma2020segmentationlossodyssey} as:
\[ L_{Dice}=1-\frac{2\sum_{i=1}^{N}{g_i*s_i}}{\sum_{i=1}^{N}{g_i^2}+\sum_{i=1}^{N}{s_i^2}} \]
where $g_i$ is the binary indicator for the foreground and $s_i$ is the probability of assigning the pixel to the foreground class.

The final loss is the combination of $L_{BCE}$ and $L_{Dice}$:

\[ L_{final} = 0.5 L_{Dice} + 0.5 L_{BCE} \]

We used a linear combination of the losses, as mentioned in \parencite{ma2020segmentationlossodyssey}.

\section{Optimisers, learning rates, regularisation}

During the development of the training loop, we observed that setting the learning rate to 0.0001 and weight decay to 0.0001 provided a stable learning. As such, we maintained a constant learning rate during training and did not experiment with these values due to time constraints. We chose to use an adaptive gradient algorithm for training the neural networks called AdamW, which is a variant of the commonly used Adam optimiser.

Regularisation is a common technique in machine learning that reduces overfitting by discouraging values with large magnitudes \parencite{Bishop2024DeepLearning}. One of the standard regularisation techniques in deep learning is weight decay, also known as L2 normalisation, which penalises large weights [TODO check]. However, while L2 normalisation and weight decay are the same, this is not true for the Adam optimiser. The authors argue that using their version, called AdamW, achieves better generalisation performance as it properly decouples the weight decay \parencite{loshchilov2019}. Since we have a sparse dataset, we wanted to include the weight decay regularisation technique. However, no significant difference in the training performance was observed.

\section{Augmentations}

A common problem with training neural networks is their capacity to overfit on the training part of the dataset. This often leads to poor generalisation performance. This is especially true for tiny datasets such as ours. One of the common ways to address this problem, as mentioned in the original U-Net paper, is the use of augmentations \parencite{Ronneberger2015} to enlarge the training set artificially. In our case, we decided to use only augmentations that do not require interpolation. The reason is that, unlike many other tasks, we are potentially dealing with relatively thin structures, which could be easily suppressed or blurred and become invisible with improper interpolation.

In each batch, a different set of transforms is used. This works by setting a random probability for this particular transform to be applied. For the training we are:

\begin{itemize}
    \item{Adding random noise with a Gaussian distribution of mean 0 and 0.01 standard deviation with probability 0.5, which adds very faint noise. This is a relatively low amount of noise and was chosen during the development of the network as we did not want to change the images to look completely different to the original ones.}
    \item{Random flipping in every axis.}
    \item{Random spatial crop which always crops to the preset size of $7 \times 64 \times 64$ from any other size.}
\end{itemize}

A potential issue with using small tiles centred at tunnels is that we may introduce unwanted bias into the network, as it might learn to always try to find the tunnel in the middle. The design designation behind using random spatial crop was to try to suppress this, as the program now randomly finds a crop of the wanted size, which reduces this problem.

\section{Training loop}
\label{sec:trainingloop}
Overall, the network training loop is shown in Algorithm \ref{alg:training}.

\begin{algorithm}
    \small  
    \caption{Quadrant-based Model Training}
    \setlength{\lineskip}{-2pt}        % Reduce line spacing
    \setlength{\baselineskip}{8pt}      % Reduce baseline spacing
    \setlength{\parskip}{0pt}           % Remove paragraph spacing
    \label{alg:training}
    \KwData{Data, Model description}
    \KwResult{Four trained models and evaluations.}

\For{$quad \in quadrants$}{
    $Reseed()$\;
    
    \tcp{Data preparation}
    $full\_train\_data, full\_test\_data \gets LoadQuadrantData(data, quad)$\;
    $mean, std \gets ComputeStats(full\_train\_data)$\;
    $train_{set}, val_{set} \gets SplitData(full\_train\_data, ratio=1/3)$\;
    
    \tcp{Model training with early stopping}
    $model \gets CreateNeuralNetwork(model\_description)$\;
    $best_{loss} \gets \infty$\;
    
    \For{$epoch \gets 1$ \KwTo $1000$ \textbf{or until} $val\_loss$ hasn't improved for 50 epochs}{
        $loss_{train} \gets TrainEpoch(model, train_{set}, mean, std)$\;
        $loss_{val} \gets ValidateEpoch(model, val_{set}, mean, std)$\;
        
        \If{$loss_{val} < best_{loss}$}{
            $best_{loss} \gets loss_{val}$\;
            $SaveBestWeights(model)$\;
        }
    }
    
    $LoadBestWeights(model)$\;
    $SaveFinalModel(model, quad)$\;
    $EvaluateModel(model, full\_test\_data)$\;
}
\end{algorithm}


\section{Inference on large images}
\label{sec:runningthenetwork}

All our networks are trained on square crops of size $7 \times 64 \times 64$ voxels, but the original images are larger at $7 \times 512 \times 512$ voxels. This is solved by using an overlap-tiling strategy similar to the one by Ronneberger et al. \parencite{Ronneberger2015}. The original images are split into tiles first, with a configurable amount of overlap in pixels. The network sequentially processes the tiles and then stitches them together to create the final prediction. The predictions are then thresholded with a value of 0.5.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/overlaps_and_processing/quad3.png}
        \caption{}
        \label{fig:stitchingoverlaporig}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/overlaps_and_processing/quad3_10px_postprocessed.png}
        \caption{} 
        \label{fig:stitchingoverlappost}
    \end{subfigure}

    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/overlaps_and_processing/quad3_0px_threshold_0_circles.png}
        \caption{}
        \label{fig:stitchingoverlap0}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{resources/overlaps_and_processing/quad3_10px_threshold_0.png}
        \caption{}
        \label{fig:stitchingoverlap10}
    \end{subfigure}

    \caption{\ref{fig:stitchingoverlaporig} shows a crop of the dataset which has been contrast-adjusted, and the \ref{fig:stitchingoverlappost} shows neural network prediction with 10px overlap between tiles and postprocessing. The second row illustrates the impact of tile overlaps on the network's thresholded prediction. In \ref{fig:stitchingoverlap0}, no overlap was used, and possible edge artefacts are emphasised with blue circles, while overlap with 10 px is used in \ref{fig:stitchingoverlap10}.}

    \label{fig:stitchingoverlap}
\end{figure}

The idea of using a configurable overlap size originated from the following observation. In Figure \ref{fig:stitchingoverlap0}, several sharp artefacts can be seen at tile borders, possibly because only part of the structure is visible, leading the network to classify it as a tunnel. This led us to the idea that if we use larger overlaps and aggregate them, these effects on tile borders will get suppressed. As can be seen in Figure \ref{fig:stitchingoverlap10}, using a larger overlap suppresses some of these artefacts. For aggregating the overlaps, a simple averaging was chosen due to its simplicity.

% todo in discussion mention that maybe this was not the best idea, maybe we
% should have chosen larger context. The context x precision tradeoff.
% TODO: consider drawing there

Even after this step, some leftover artefacts may remain, some of which are linked to noisy input, while others serve as evidence that it is not a perfect solution to the artefacts caused by partial context. Some of these false TNTs at the border of tiles are relatively small compared to real tunnels, which leads us to the final piece of the inference - the postprocessing.

\section{Postprocessing}
\label{sec:postprocessing}

As mentioned in the previous section, even after applying the overlap strategy, some artefacts remain. Moreover, some artefacts actually closely resemble thin structures but are not true tunnels. Many of these false tunnels are small and not connected to any cell, nor are they significantly present at multiple depth slices. Therefore, an easy way to filter would be by using their volume.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/histogramofsizes.png}
    \end{center}
    \caption{Histogram of the volume (in voxels) of all labelled tunnelling
    nanotubes with median and mean highlighted.}.
    \label{fig:histogramoflabelsizes}
\end{figure}

The value of 100 pixels, realising that some of the tiny tunnels might get removed, but as will be shown later, the network tends to find more tunnels than the biologist. Hence, I believe this to be a reasonable assumption to make. The Figure \ref{fig:histogramoflabelsizes} shows a histogram of the distribution of labels' volumes. Removing components with a volume smaller than 100 voxels removed just a small number of all tunnels.

\chapter{Implementation}

The high-level logic of the training loop is discussed in Section \ref{sec:trainingloop}, and the inference is discussed in \ref{sec:runningthenetwork}. This chapter addresses some of the technical details of the implementation. The complete code, written in Python and Bash, is available in the thesis's archive along with several trained networks and a full user's reference.

Although the Python programming language is an interpreted and dynamically typed language, and therefore is comparatively slower than other lower-level languages traditionally used in image processing, such as C++, its nature allows it to rapidly prototype new applications. Moreover, many packages call code written in other languages, such as C++, Fortran or C, to alleviate the performance problem. Lastly, Python has become a widely adopted language in the area of neural networks, and many researchers publish their code in Python; thus, Python was chosen to handle the implementation part.

As much of the evaluation involves proper cross-validation and averaging the results, it was easier to create a script that can be easily configured using a text editor or by providing and parsing CLI arguments. The averaging and selecting different datasets should not be the responsibility of the Python script. As such, creating directories, running code, aggregating results and parsing arguments is a natural job for a scripting language such as Bash. While this obviously limits the project's use to Unix-like operating systems, this is acceptable, as many computational servers are now running on Linux, and the trained models are OS-agnostic.

The overall structure of the project code is as follows: a single TNTSegment package is available, which handles data loading, data splitting, and defining the neural networks and their modules. Outside of the package, the training script, evaluation script, and postprocessing script are available. Finally, to connect it all, running the automated training pipeline that respects the cross-folds and aggregates the results involves a mix of Bash and Python scripts. In-depth instructions as to how to run these scripts are available alongside the code in the thesis archive.

As for the other tooling, here is the list of the most important tools:
\begin{itemize}
    \item{MLflow is a tool for handling machine learning pipelines. It allows for tracking experiments, comparing results, saving and loading models, deploying models, and more. In this work, it's nice to use UI was used to keep track of the experiments \parencite{Zaharia2018MLflow}}
    \item{NumPy has become one of the fundamental packages in scientific computing in the Python ecosystem. It provides a very efficient implementation of many common operations on N-dimensional arrays. \parencite{Harris2020Numpy}}
    \item{scikit-learn provides a great framework for building machine learning models, not just deep learning models. The library was primarily used for splitting datasets and utilising its framework for handling batching. \parencite{Pedregosa2011scikit}}
    \item{scikit-image is a package providing various algorithms for processing images, e.g. segmentation algorithms, point transform, and mathematical morphology. I used it to handle the postprocessing of masks and to find connected components.\parencite{vanderWalt2014skimage}}
    \item{PyTorch is a popular deep learning framework with a focus on flexibility and speed. This enables users to create highly performing neural networks quickly. It automatically creates computational graphs, allowing it to compute gradient updates efficiently. It automatically handles connecting to various hardware, such as CPUs and GPUs. These advantages made PyTorch one of the most used packages in the Python machine learning ecosystem \parencite{Ansel2024PyTorch}.}
    \item{MONAI is another open-source framework for building neural networks on top of PyTorch. It focuses on building models used in the medical area. In the project, this package handles augmentations of the dataset, providing first-class support for 3D data, unlike PyTorch. \parencite{Cardoso2022Monai}}
    \item{Matplotlib is a library for visualising data in an interactive fashion. The vast majority of graphs in this text were created using it. \parencite{Hunter2007Matplotlib}}
    \item{Pandas is a popular library for handling data. It provides a DataFrame object for efficient data manipulation, reshaping, and indexing, among many other things. In this work, the final evaluations were created using the Pandas library. \parencite{Pandas}}
\end{itemize}
Along with these tools, a formatter called Ruff, a tifffile package for reading and writing images, and torchinfo package for benchmarking the number of parameters of any model were also used.

\section{Environment}
Handling dependencies in Python can become tricky, especially for reproducing the environments. For this purpose, the environment's dependencies and building a virtual environment are resolved by a tool called \texttt{uv} \parencite{uv2025}, developed by Astral. The developer uses a TOML file to specify project dependencies. This file is then used by \texttt{uv} to recreate the environment.

For convenience, I am also including a \texttt{requirements.txt} file, which was automatically generated from a virtual Python environment; however, the officially supported version uses the \texttt {uv} package manager. The details of recreating the environment are described in the attached file \texttt{README.md}.

\section*{Various implementation details}
While reviewing the code, the reader may encounter the following observations, which I believe are worth mentioning and explaining in more detail.

While the implementation of CSAM required only minor changes to correctly permutate the dimensions of the input, SE blocks were programmed from scratch. I took the PyTorch implementation of 2D SE and replaced the operations with their 3D equivalents. Mainly, the squeeze operation is a 3D average pooling layer, and the linear layers are replaced with convolution layers followed by \texttt{ReLU}. The final difference compared to the description available in Section \ref{sec-seblock} is that the first convolutional operator in the excitation operation reduces the number of channels from $C$ to $C/r$, where $r$ is called a reduction factor. Then the \texttt{ReLU} is run, and the final convolution operator expands the number of channels back to $C$. The purpose of this reduction factor was explored in \parencite{Hu2018}, where they studied the tradeoff between performance and computational cost. We used the number $16$ for the value, as is suggested by the authors.

While reviewing the training loop, you may notice mentions of determinism and seed setting. The effect of seeds and deterministic algorithms in PyTorch was briefly investigated while fixing various bugs in the training loop. Currently, the user must manually seed the random number generator; however, the training still exhibits random behaviour.

While running the training and evaluations, many more metrics are calculated beyond the Dice score presented in later chapters. They are still present, but were not used in the final evaluations.

\chapter{Evaluation}
This chapter discusses how the different networks were evaluated on the entire dataset and presents the results of an ablation study designed to set the values of the depth and overlap hyperparameters. The chapter ends with performance benchmarks across the architectures.

\section{Metrics}
% todo : consider citation
For evaluating the resulting predictions, we decided to use Dice scores as they are commonly used in many biomedical papers, but the end user is not interested in the quality of the scores on cutouts around tunnels, or how every neural network was trained; it is crucial to know how the neural network performs on the whole image.

As described in Section \ref{sec:runningthenetwork}, the images are tiled and then stitched together with a configurable amount of overlap. Later in Section \ref{sec:postprocessing}, the postprocessing of the predictions is discussed. The first two metrics we will discuss are calculated as the Dice scores of the binarised output (with and without preprocessing) and the binary reference annotations. Furthermore, we will conduct a detailed analysis of segmentation performance, particularly focusing on how well the network identifies and delineates individual tunnels within the images. 

\subsection*{Mapping}
\label{sec:mapping}

We define a mapping between the predictions and the tunnels in the reference annotation to investigate performance at the tunnel level and the performance on clearly identified tunnels, excluding false positives (FPs).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/inkscape/matching_simple.png}
            \put(8,85){\Large\textbf{(a)}}
        \end{overpic}
        \label{fig:matchingsimple}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/inkscape/matching_multiple.png}
            \put(8,85){\Large \textbf{(b)}}
        \end{overpic}
        \label{fig:matchingmultiple}
    \end{subfigure}
    \caption{Three vertical pillars, shown as black circles, are connected by TNTs, also shown as black curves. In (a) the network found two tunnels. Identifying a real tunnel correctly is considered a true positive (TP) and is shown in blue, while the second prediction, in red, is a false positive. In (b), the network predicts a single tunnel, which covers both real tunnels. This is also considered to be a TP.}
    \label{fig:matchingdiagram}
\end{figure}

In Figure \ref{fig:matchingdiagram} we can see a schematic of a situation with three pillars and several predictions. In (a) a network predicted a single TP, FP, and FN.
Whereas in (b), a situation where a single prediction covers both tunnels is more complicated. This might be present near a cluster of pillars and tunnels, where a clear separation is difficult to see. The tunnels might theoretically even cross each other. While discarding these results is possible, we will consider such a situation a success and label it as finding two TPs. A situation where two predictions cover a label is not investigated.

Mathematically, the mapping is implemented by examining the tunnels in the following manner. After the postprocessing defined in Section \ref{sec:postprocessing} the predictions form a set $P$, whose elements are the binary masks of each connected component. Likewise, we can view the reference annotations as a set $L$, where each element is also a binary mask for each instance of a tunnel. Given these two sets, we define a mapping between the two sets as a set of tuples $(l,p)$, where $l$ and $p$ form a pair describing which predicted component matched a tunnel in the reference annotation. The set of all possible matches is defined as:

\[ \text{mapping}(P,L)=\{(l,\hat{p})|\forall l \in L, \hat{p} = \text{argmax}_{p \in P} \text{recall}(p,l) \land \text{recall}(\hat{p},l) > 0.5 \} \]

The deciding factor is how much of the label is covered by the prediction, which is equivalent to the recall score, and we set it to 0.5, as at least half of the label must be covered. This definition formalises the mapping shown in Figure \ref{fig:matchingdiagram}.

Lastly, we would like to investigate so-called clean matches, or 1-1 matches, where a prediction matches just a single label:

\[ \text{clean\_mapping} = \{ (l, p) | p \text{ matches just single label} \} \]

These definition allows us to talk about the result in Figure \ref{fig:matchingmultiple} in two ways. When considering all matches, the prediction $p_1$ covers both tunnels; however, when examining clean matches, no matches are found.

The defined matching enables us to quantify how well the network performs at the pixel level, as well as in terms of identifying individual instances of tunnels.

\subsection*{All metrics}

Now we will define the full set of Dice scores, which will be evaluated for each architecture:

\begin{itemize}
    \item{Test(pixel level): Look at the performance on crops of tunnels.}
    \item{Evaluation (pixel level): Look at the performance derived from the overall stitched binary images}
   \item{Postprocessed overall (pixel level): Look at the performance derived from the overall stitched binary images after postprocessing}
\end{itemize}

Next, we will use the matches defined above. In these cases, the matches are taken, and a binary image is created by aggregating the binary sets using a binary logical OR operator. This means that for each pixel, if any of the corresponding pixels in the matches are true, the pixel in the resulting binary image will also be true. This aggregation process enables us to create a single comprehensive binary image that captures all the features represented by the matches, effectively highlighting the areas of interest identified earlier, ignoring artefacts caused by false positives and false negatives.

\begin{itemize}
    \item{Postprocessed matches (pixel level): Look at the performance derived from all the matches.}
    \item{Postprocessed clean matches (pixel level): Look at the performance of just the clean matches.}
    \item{Tunnel metric (tunnel level): Here the performance is calculated from the tunnels instead of pixels.}
\end{itemize}
% todo: as an example, show the quad with all sorts of tunnels connected together.

\section{Ablation study}
\label{sec:ablationstudy}
In this section, we present experimentally inferred sizes of hyperparameters for the 3D variant of AnisoUNet.

It is not apparent how to set the tiling overlap, but the choice of this hyperparameter affects every neural network we have discussed. Also, every advanced neural network built on top of the AnisoUNet network, described in Section \ref{sec:anisounet}, has configurable model depth.

% TODO: think about citing something
In general, it is accepted that the ability to teach deeper networks helped improve neural networks' performance and, in some sense, the ability to teach deeper networks led us to the neural network revolution. Nevertheless, the size of the network should be connected to the complexity of the problem, both in terms of the size of the context and the image complexity. To find a reasonable value for these parameters, we decided to do a 2D hyperparameter search study. 

% todo: talk about this in the discussion

Taking the aforementioned AnisoUNet3D architecture, we considered five different depths. The shallowest one has exactly two downsampling layers, so it is referenced as a network with depth equal to 2, and the deepest one has six downsampling layers. Six downsampling layers were chosen, as this is sufficient to reduce the original size of 7x64x64 to 7x1x1; therefore, a deeper network is not necessary.

For the overlap hyperparameter, we chose the values 0px, 10px, 20px, 30px, and 40px. The value 40px was chosen arbitrarily, as in this case, the neighbouring cells already share more than half of the volume.

As the overlap is a hyperparameter used during the inference on the whole images, this means that only five different architectures must be trained to perform this analysis. Since we are doing 4-fold cross-validation, we are training 20 models. Finally, since neural networks employ randomness, each combination of a fold and architecture is repeated three times, leading to training 60 models for this hyperparameter study.

Figure \ref{fig:parametersearchstudy} showcases the Dice score of the various networks. Every number is the mean performance across the 4-fold cross-validation and three independent runs. Overall, the highest number is highlighted, corresponding to the version with three downsampling layers and a 10px overlap. Therefore, all subsequent models will use these values for the hyperparameters.

% TODO: tripple check, i had very different numbers before

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.7\textwidth]{resources/evaluations/eval_dice_mean_heatmap.png}
    \end{center}
    \caption{Evaluation Dice score for various depths of the network. The best value is highlighted in yellow.}
    \label{fig:parametersearchstudy}
\end{figure}

What we can also see from this hyperparameter search is that the effect of the number of downsampling layers is rather marginal. Another effect that is visible is that while changing between 10 pixels to 20 or 30 pixels does minimal change in the performance, the difference between 0 pixels and all others is visible in every case. Also, it is interesting to see that although the changes are not significantly different, the network with three downsampling layers consistently beat the others.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{resources/evaluations/eval_dice_mean_architecture_comparison.png}
    \caption{Evaluation metric. Each bar represents the average of twelve model runs across the 4-fold cross-validation. Error bars represent the sample standard deviation.}
    \label{fig:evaldepth}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{resources/evaluations/postprocess_overall_dice_architecture_comparison.png}
    \caption{Postprocess overall metric. Each bar represents the average of twelve model runs across the 4-fold cross-validation. Error bars represent the sample standard deviation.}
    \label{fig:postprocessoveralldepth}
\end{figure}

In Figure \ref{fig:evaldepth}, the effect of the depth is studied further on the Evaluation metric. Figure \ref{fig:postprocessoveralldepth} shows the Postprocessed overall metric.

First of all, the performance differences across various depths are not that substantial, but overall, the network with three downsampling steps and the network with five downsampling steps are achieving the highest averages consistently, whereas the networks with two and six downsampling steps are performing the worst. This can be explained by the fact that the deepest network is unreasonably large for our given problem, while the smallest network might suffer from not enough layers.

However, it must be noted that the error bars are large and complicate any conclusion, as they show that the performance varies quite dramatically. This led us to investigate what could be causing this variety, and one possible explanation is the various performances on folds.

Taking a look at Figure \ref{fig:evalquadsdepth} here, we can see the same metrics, but this time it is further divided among the various quadrants on which the network was tested. What is immediately evident to us is that the error bars are less varied, due to the performance differences across quadrants. Interestingly, in some cases, such as the models with size downsampling layers for the first quadrant, there are significant performance differences, but no apparent reason for this was found. Since, in this case, all bars represent averages of three numbers, it is plausible that, due to bad luck, the network may not achieve as good a performance as in other runs. Another possible reason is that the deeper variants have significantly more parameters and are overfitting on the sparse dataset.
% TODO talk in discussion

In general, all models performed the best at the third quadrant (top left) and the third quadrant (bottom left), while the performance was lower on the second quadrant (top right) and the fourth quadrant (bottom right). Looking at the means and error bars now, it seems that the model with three downsampling layers performs best or close to best in all quadrants, whereas having more layers can sometimes reach similar performance it might also lead to worse (like in case of three downsampling steps in quadrant 3); hence, we cannot say that for our input size, the increased depth helps dramatically. However, our previous decision to choose the network of depth three in the following studies still stands, as although the performance difference is not significant, this network consistently outperforms the smaller network, and at the same time, has many fewer parameters than the larger networks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{resources/evaluations/eval_dice_mean_by_quad_arch.png}
    \caption{Evaluation metrics for various folds. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:evalquadsdepth}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{resources/evaluations/postprocess_overall_dice_by_quad_arch.png}
    \caption{Postprocess overall metric for various folds. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:postprocessoverallquadsdepth}
\end{figure}

\section{Different architectures}
\label{sec:diff-architectures}

As we have discussed in the previous section, we did a 2D hyperparameter search to choose a reasonable estimate of the depth and overlap values, since these are used by our advanced neural networks. We picked three downsampling layers and an overlap of 10 pixels. In this section, the evaluation of other networks is presented.

We decided to study the following architectures: a basic implementation of UNet called BasicUNet as described in Section \ref{sec:unet}. The anisotropic version AnisoUNet as described in Section \ref{sec:anisounet}, a version of AnisoUNet with the CSAM module as described in Section \ref{sec-csammodule}, and finally the UseNet variant of AnisoUNet architecture from Section \ref{sec-seblock}. All models use a 10 pixel overlap, and their depth is equal to three downsampling blocks, with the exception of BasicUNet, which has two downsampling blocks. For every AnisoUNet variant, both 3D and 2D versions were trained, as is described in Section \ref{sec:anisounet}.

\subsection*{Metrics}

Moving on to the results, Figs. \ref{fig:eval-diff-arch-dice}- \ref{fig:eval-diff-arch-tunnel} show the same metrics we discussed in Section \ref{sec:ablationstudy} aggregated across all the cross folds, while \ref{fig:eval-diff-arch-dice-quads}-\ref{fig:eval-diff-arch-tunnel-quads} show the performance for each quadrant. The means and standard deviations of all metrics were calculated exactly the same as in the previous section; again, every combination was trained three times.

We can again see large error bars in all figures, especially when looking at the tunnel metrics in Figure \ref{fig:eval-diff-arch-tunnel}. Like in the previous section, this is caused by the difference in performance across the folds and will be discussed after analysing the averages across folds.

We can clearly see that the Basicunet architecture is performing worse in every metric than the rest of the architectures, which points to the fact that the reduced depth and isotropic kernels are not suitable for highly anisotropic data such as ours, and shows that anisotropic kernels in the pooling operations greatly help the network learn.

While it is difficult to assess the overall trends due to the error bars, the various attention mechanisms do not provide much improvement for the AnisoUNet neural network. The average values of the evaluation metric for the CSAM and USENet variants are slightly higher for the 3D versions, while they lower the average performance in the 2D case. Moreover, the 2D version of AnisoUNet slightly outperforms the 3D version. The same is happening for the AnisoUNet-CSAM variant, but it is not true for the UseNet variant.

Moving to the advanced metric, we can observe that postprocessing is slightly improving the Dice score, indicating that the removal of all small components helps eliminate artefacts. While comparing the Figs \ref{fig:eval-diff-arch-clean} and \ref{fig:eval-diff-arch-all}, we can see that the networks reach higher performance when looking at the correctly identified tunnels. This means that the tunnel's accuracy is hindered by finding false tunnels and/or not fully segmenting real tunnels. It is also clear that the difference in performance between the cleanly matched and all matches is not substantial.

Finally, the Tunnel metric shows that the performance is even more varied across the folds. Interestingly, the 2D versions have a higher average performance than the 3D versions, but at the same time, they exhibit much greater variation in their performance.

Moving to the evaluations per quadrant, in Figure \ref{fig:eval-diff-arch-dice-quads}, we can see that all neural architectures perform the best on the first and third quadrant, whereas the second quadrant is the most problematic. There does not seem to be a clear winner between the architectures, as a particular attention mechanism outperforms the standard AnisoUNet in one quadrant but is then surpassed in another. Lastly, the difference between the 2D and 3D variants is not significant in most quadrants; they differ significantly in the second quadrant, where the 2D versions outperform the 3D ones.

Looking at the advanced metrics, the overall Dice metric in Figure \ref{fig:eval-diff-arch-dice-overall-quads} shows that, again, the postprocessing consistently improves the performance of all models. Moving to the cleanly matched in Figure \ref{fig:eval_diff_arch_clean_match_dice_quads} and all matches in Figure \ref{fig:eval_diff_arch_matched_dice_quads}, it is clear that the networks are achieving significantly better performance compared to the overall score. Again, this suggests that the neural networks have issues with noise and false positives. Interestingly, the noteworthy performance difference between the second quadrant and the rest seems to disappear here. A possible explanation is that while the neural network performs similarly on clear data, its ability to handle noise and differentiate between real tunnels and artefacts varies. Lastly, the tunnel metric in Figure \ref{fig:eval-diff-arch-tunnel-quads} shows that the networks struggled to correctly identify what is and what is not a tunnel in the second quadrant. The best performance was achieved by the 2D version of AnisoUNet, reaching a Dice score of $0.772$. Again, it appears that the 2D version tends to outperform the 3D version in this task as well. It is also noteworthy that in this metric, the performance of a given network can vary significantly, as in the case of 2D UseNet in the third quadrant. No clear explanation was found, and it is likely caused by random fluctuations in the training performance, as these bars were generated from three values.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{resources/evaluations/diffarchs/eval_dice_mean_architecture_comparison.png}
    \end{center}
    \caption{Evaluation metrics for various folds. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-dice}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{resources/evaluations/diffarchs/postprocess_overall_dice_architecture_comparison.png}
    \end{center}
    \caption{Postprocess overall metric. Each bar represents the average of twelve model runs across the 4-fold cross-validation. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-overall}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{resources/evaluations/diffarchs/postprocess_clean_matched_dice_architecture_comparison.png}
    \end{center}
    \caption{Dice metric on clean matches. Each bar represents the average of twelve model runs across the 4-fold cross-validation. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-clean}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{resources/evaluations/diffarchs/postprocess_matched_dice_architecture_comparison.png}
    \end{center}
    \caption{Dice metric on all matches. Each bar represents the average of twelve model runs across the 4-fold cross-validation. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-all}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{resources/evaluations/diffarchs/tunnel_dice_architecture_comparison.png}
    \end{center}
    \caption{Dice metric on clean matches. Each bar represents the average of twelve model runs across the 4-fold cross-validation. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-tunnel}
\end{figure}


\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/evaluations/diffarchs/eval_dice_mean_by_quad_arch.png}
    \end{center}
    \caption{Evaluation metric. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-dice-quads}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/evaluations/diffarchs/postprocess_overall_dice_by_quad_arch.png}
    \end{center}
    \caption{Overall dice metric. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-dice-overall-quads}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/evaluations/diffarchs/postprocess_clean_matched_dice_by_quad_arch.png}
    \end{center}
    \caption{Dice metric on clean matches. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:eval_diff_arch_clean_match_dice_quads}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/evaluations/diffarchs/postprocess_matched_dice_by_quad_arch.png}
    \end{center}
    \caption{Dice metric on all matches. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:eval_diff_arch_matched_dice_quads}
\end{figure}
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{resources/evaluations/diffarchs/tunnel_dice_by_quad_arch.png}
    \end{center}
    \caption{Tunnel dice metric. Each bar represents the average of three model runs. Error bars represent the sample standard deviation.}
    \label{fig:eval-diff-arch-tunnel-quads}
\end{figure}

\subsection*{Benchmarks}
The neural network was trained and run on a machine at the faculty with AMD EPYC 7702P 64 Core CPU, 504 GB of memory, and the A100-SXM4-80GB Nvidia GPU. Table \ref{table:benchmarks} shows the inference time of the network and memory requirements. The number of parameters and the estimated size were gathered using the \texttt{torch-info} package, while the peak memory time was gathered using \texttt{PyTorch} utilities.

\begin{table}[htbp]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Architecture} & \textbf{Parameters} & \textbf{peak VRAM [MB]} & \textbf{inference time [ms]} & \textbf{throughput [samples/sec]} \\
        \hline
        Aniso(2D) & 7701825 & 1578.11 & 21.036 & 1521.2 \\ \hline
        Aniso(3D) & 21711297 & 1689.06 & 35.6926 & 896.55 \\ \hline
        UseNet(2D) & 7735509 & 1578.38 & 22.847 & 1400.62 \\ \hline
        UseNet(3D) & 21744981 & 1689.32 & 37.5545 & 852.1 \\ \hline
        CSAM(2D) & 8259587 & 1647.08 & 25.4008 & 1259.8 \\ \hline
        CSAM(3D) & 22269059 & 1757.44 & 39.9889 & 800.22 \\ \hline
        BasicUNet & 5420737 & 1449.24 & 18.2588 & 1752.58 \\ \hline
    \end{tabular}
    }
    \caption{Memory benchmarks and inference times for the investigated networks. Each inference time is an average of 200 runs on random data of shape $7\times 64 \times 64$ with batch size 32. A sample corresponds to a single tile.}
    \label{table:benchmarks}
\end{table}

\subsection*{Qualitative study}
Lastly, we will investigate the performance in a qualitative manner. We will start by analysing the smaller performance in the second quadrant, then we will look at issues with large variance for the third quadrant in the UseNet 2D variant, and finally we will look at how the neural network handles parts with a relatively small amount of noise.

Figure \ref{fig:qualitative-quad2} shows the neural network prediction for various architectures. In the previous section, we showed that the performance in this quadrant is lower, which is caused by the networks' mistaking visual artefacts in the top right part of the image for TNTs. As similar artefacts are not present in other parts of the dataset, the neural network fails to learn to distinguish this. Interestingly, it can be seen that the 2D version handles this better than the 3D version, which could be caused by the fact that this particular artefact is present in multiple slices, confusing the network.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad2eval/t017-composite-slice7.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(a)}}}
        \end{overpic}
        \label{fig:quad2orig}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad2eval/t017-slice7-basicunet.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(b)}}}
        \end{overpic}
        \label{fig:quad2basicunet}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad2eval/t017-slice7-csam3d.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(c)}}}
        \end{overpic}
        \label{fig:quad2csam3d}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad2eval/t017-slice7-csam2d.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(d)}}}
        \end{overpic}
        \label{fig:quad2csam2d}
    \end{subfigure}
    \caption{Predictions on the second quadrant. (A) shows the original image (contrast-enhanced) with binarised reference annotations in magenta. (B) shows the prediction of BasicUNet, (C) 3D version of CSAM variant, (D) shows the 2D version of CSAM. Green represents TP, red stands for FP and blue is for FN.}
    \label{fig:qualitative-quad2}
\end{figure}

Continuing with the analysis in the third quadrant, where the 2D UseNet performance varied quite significantly. The Figure \ref{fig:qualitative-quad3} shows that due to probably bad luck during training, one of the models (reaching a tunnel dice score of 0.7) managed to correctly match two of the three tunnels, while the last one failed to match any (tunnel dice score is about 0.47), due to the poor prediction shown in the blue colour. The poorer prediction on the stitched images lead to missed opportunities with matching.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad3eval/t017-ce-slice7-gimp-crop.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(a)}}}
        \end{overpic}
        \label{fig:quad3gimp}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad3eval/nice-slice7-crop.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(b)}}}
        \end{overpic}
        \label{fig:quad3nice}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad3eval/nice-slice7-crop.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(c)}}}
        \end{overpic}
        \label{fig:quad3bad}
    \end{subfigure}
    \caption{Predictions on part of the third quadrant. (A) shows the original image (contrast-enhanced) with reference annotations. Different colours correspond to different tunnels. (B) and (C) show predictions of two different models. Red means that a tunnel from reference was not matched to any predicted tunnel, while blue means that a predicted tunnel did not match any reference tunnel.}
    \label{fig:qualitative-quad3}
\end{figure}

In the final study, we took a look at the first quadrant, which reached the highest scores. In Figure \ref{fig:qualitative-quad1}, a part of the dataset with minimal artefacts is shown. As can be seen, our networks tend to have a problem with oversegmentation and mark small structures as tunnels even though they are not. While the CSAM and UseNet variants do not improve the prediction by AnisoUNet, the prediction by AnisoUNet looks decent.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad1eval/t000-composite-slice-1.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(a)}}}
        \end{overpic}
        \label{fig:quad1orig}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad1eval/aniso2d-slice1.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(b)}}}
        \end{overpic}
        \label{fig:quad1aniso}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad1eval/usenet2d-slice1.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(c)}}}
        \end{overpic}
        \label{fig:quad1usenet}
    \end{subfigure}
    \begin{subfigure}{0.40\textwidth}
        \centering
        \begin{overpic}[width=\textwidth]{resources/qualitative/quad1eval/csam2d-slice-1.png}
            \put(8,85){\Large  \textcolor{white}{\textbf{(d)}}}
        \end{overpic}
        \label{fig:quad1csam}
    \end{subfigure}
    \caption{Predictions on the first quadrant. (A) shows the original image (contrast-enhanced) with reference annotations in magenta. (B) shows the prediction of 2D AnisoUNet, (C) 2D version of UseNet variant, (D) shows the 2D version of CSAM variant. Green represents TP, red stands for FP and blue is for FN}
    \label{fig:qualitative-quad1}
\end{figure}

\chapter{Discussion and conclusion}
% what to do next
% talk about:
% - stitching x overlap size x size
% - recall threshold
% - size of network connected to the complexity
% - performance differences due to luck
% - myabe mention that the neural network could be trained using semi-supervised
%   approach?

% what could be done
% - 

This thesis successfully explored the application of U-Net-like architectures for the automated semantic segmentation of Tunneling Nanotubes (TNTs) in 3D fluorescent microscopy images of Drosophila wings. The research was conducted on a challenging dataset defined by high anisotropy and extremely sparse annotations, with ground truth labels available for only two time steps.

To overcome these significant data constraints, the work focused on three core contributions: the AnisoUNet architecture, a robust training strategy, and a complete inference system. We designed and implemented an anisotropic variant of the standard U-Net (AnisoUNet), which was crucial for effectively processing 3D images with severely restricted Z-axis resolution and enabling the training of deeper networks. This was supported by a robust 4-fold cross-validation training pipeline, a strategy essential for maximising the utility of the sparse annotations. Finally, a comprehensive and user-friendly inference implementation was provided making the segmentation tool immediately accessible to non-technical users in biological research.

Multiple architectures, including variants with attention mechanisms (CSAM and USE-Net), were evaluated. The results demonstrated the feasibility of using deep learning for segmenting these fine nanotubular structures, even under such data limitations. The achieved performance in real-world testing and across most quadrants typically ranged between 0.5 and 0.6. This indicates that while the networks can find meaningful predictions, their main limitation is a tendency to oversegment and flag various cellular protrusions as false tunnels. No single complex architecture consistently outperformed the simpler AnisoUNet across all tests, suggesting that future improvements should focus more on data quality and post-processing rather than architectural complexity.

In the future, it might be interesting to refine the post-processing e.g. using a different method to segment the pillars. Also, future research might focus on temporal tracking of these tunnels once more annotations are acquired.


\end{document}
